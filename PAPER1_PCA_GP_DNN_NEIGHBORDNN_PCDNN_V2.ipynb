{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH4-AIR Notebook \n",
    "This notebook contains all the methods and the 2 experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression#, sRidge\n",
    "from sklearn import gaussian_process\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "import scipy.optimize\n",
    "from sklearn.utils.optimize import _check_optimize_result\n",
    "from sklearn.gaussian_process.kernels import Matern, RBF, WhiteKernel, RationalQuadratic, ExpSineSquared\n",
    "import time\n",
    "from sklearn.decomposition import PCA, SparsePCA\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.utils import shuffle\n",
    "# demonstrate data normalization with sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n",
    "\n",
    "# this enables XLA devices for tensorflow\n",
    "os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# TODO: remove this if it is unnecessary\n",
    "# this limits GPU memory usage for the sake of parallel GPU processes\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "print(gpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the data into a dataframe\n",
    "df = pd.read_csv('./NewData_flames_data_with_L1_L2_errors_CH4-AIR_with_trimming.txt')\n",
    "df = df.iloc[:-1]\n",
    "\n",
    "#create an integer representation of the flame-id and add to the data frame\n",
    "df['flame_key_int'] = df[' flame_key'].mul(10000000).astype(int)\n",
    "\n",
    "#create an integer to determine if the flame is included by the framework in the manifold creation and reverselookup\n",
    "#framework_untrimmed_flameids = [0.00115982, 0.00122087, 0.00128512, 0.00135276, 0.00142396, 0.0014989, 0.00157779, 0.00166083, 0.00174825, 0.00184026, 0.00193711, 0.00203907, 0.00214639, 0.00225936, 0.00237827, 0.01]\n",
    "\n",
    "framework_untrimmed_flameids = ['2.0276547153583627E-4', '2.1343733845877503E-4', '2.2467088258818426E-4', '2.3649566588229923E-4', '2.4894280619189394E-4', '2.6204505914936203E-4', '2.7583690436774953E-4', '2.903546361765785E-4', '3.056364591332405E-4', '3.2172258856130585E-4', '3.3865535638032194E-4', '0.0032353354497370902']\n",
    "\n",
    "\n",
    "framework_untrimmed_flame_key_ints = [int(float(framework_untrimmed_flameids[i])*10000000) for i in range(len(framework_untrimmed_flameids))]\n",
    "\n",
    "def isFlame_included(flame_key_int):\n",
    "    if flame_key_int in framework_untrimmed_flame_key_ints:\n",
    "        ret_val = 1\n",
    "    else:\n",
    "        ret_val = 0\n",
    "    return ret_val\n",
    "\n",
    "df['is_flame_included_by_framework'] = df['flame_key_int'].map(lambda x: isFlame_included(x))\n",
    "\n",
    "df['souener_deciles'] = pd.qcut(df['souener'],10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Petal.Length; 4.598 +/- 0.381\n",
      "Petal.Width; 0.528 +/- 0.048\n",
      "Sepal.Width; 0.235 +/- 0.031\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# does permutation feature importance test on an arbitrary model & reports graphically/textually the results\n",
    "def do_perm_feature_importance(model, X_data, Y_data, random_state=0):\n",
    "    from sklearn.inspection import permutation_importance\n",
    "    r = permutation_importance(model, X_data, Y_data,\n",
    "                               n_repeats=30,\n",
    "                               random_state=random_state)\n",
    "\n",
    "    argsort = r.importances_mean.argsort()[::-1]\n",
    "    \n",
    "    for i in argsort:\n",
    "         if r.importances_mean[i] - 2 * r.importances_std[i] > 0:\n",
    "            print(f\"{list(X_data.columns)[i]}; {r.importances_mean[i]:.3f} +/- {r.importances_std[i]:.3f}\")\n",
    "    a = list(X_data.columns[argsort])\n",
    "    b = r.importances_mean[argsort]\n",
    "    c = r.importances_std[argsort]\n",
    "    bar = plt.bar(a, b, label='mean')\n",
    "    err = plt.errorbar(a, b, yerr=c, fmt=\"o\", color=\"r\", label='std')\n",
    "    plt.title('Model\\'s (Permutation) Feature Importance')\n",
    "    plt.legend(handles=[bar, err])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "iris = pd.read_csv('~/rom_project/iris.csv').iloc[:, 1:]\n",
    "iris = iris.select_dtypes(['number'])\n",
    "\n",
    "X_iris = iris.iloc[:,1:]\n",
    "Y_iris = iris.iloc[:,0]\n",
    "#print(X_iris)\n",
    "#print(Y_iris)\n",
    "\n",
    "lm = LinearRegression().fit(X_iris, Y_iris)\n",
    "\n",
    "do_perm_feature_importance(lm, X_iris, Y_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0s</th>\n",
       "      <th>1s</th>\n",
       "      <th>2s</th>\n",
       "      <th>3s</th>\n",
       "      <th>4s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.289924</td>\n",
       "      <td>0.513861</td>\n",
       "      <td>0.759479</td>\n",
       "      <td>0.197186</td>\n",
       "      <td>0.952499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.441984</td>\n",
       "      <td>0.406293</td>\n",
       "      <td>0.723578</td>\n",
       "      <td>0.559638</td>\n",
       "      <td>0.977708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.837521</td>\n",
       "      <td>0.206182</td>\n",
       "      <td>0.429881</td>\n",
       "      <td>0.146572</td>\n",
       "      <td>0.536963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.999896</td>\n",
       "      <td>0.786400</td>\n",
       "      <td>0.006285</td>\n",
       "      <td>0.447210</td>\n",
       "      <td>0.337804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.246840</td>\n",
       "      <td>0.723121</td>\n",
       "      <td>0.938789</td>\n",
       "      <td>0.360412</td>\n",
       "      <td>0.024222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0s        1s        2s        3s        4s\n",
       "0  0.289924  0.513861  0.759479  0.197186  0.952499\n",
       "1  0.441984  0.406293  0.723578  0.559638  0.977708\n",
       "2  0.837521  0.206182  0.429881  0.146572  0.536963\n",
       "3  0.999896  0.786400  0.006285  0.447210  0.337804\n",
       "4  0.246840  0.723121  0.938789  0.360412  0.024222"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "cols = list(map(lambda x: str(x)+\"s\", range(5)))\n",
    "x = np.random.uniform(size=[5,5])\n",
    "\n",
    "pd.DataFrame(data=x, columns=cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create PCAs using all the data and add to DF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_principal_components = 5\n",
    "\n",
    "pca = PCA(n_components=num_principal_components)\n",
    "\n",
    "icovariates = []\n",
    "for c in df.columns:\n",
    "    if c[0:2] == 'Yi':\n",
    "        icovariates.append(c)\n",
    "\n",
    "X = df[icovariates].values\n",
    "        \n",
    "pure_pca_dim_cols = [\"PURE_PCA_\"+str(i+1) for i in range(num_principal_components)]\n",
    "\n",
    "pca.fit_transform(X)\n",
    "        \n",
    "df_pure_pca = pd.DataFrame(pca.transform(X), columns = pure_pca_dim_cols)\n",
    "        \n",
    "df = pd.concat([df,df_pure_pca], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Sparse PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparsepca = SparsePCA(n_components=num_principal_components)\n",
    "      \n",
    "sparse_pca_dim_cols = [\"SPARSE_PCA_\"+str(i+1) for i in range(num_principal_components)]\n",
    "\n",
    "sparsepca.fit_transform(X)\n",
    "        \n",
    "df_sparse_pca = pd.DataFrame(sparsepca.transform(X), columns = sparse_pca_dim_cols)\n",
    "        \n",
    "df = pd.concat([df,df_sparse_pca], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create PCAs orthogonal to Zmix and add to DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "zmix_pca_dim_cols = [\"Zmix_PCA_\"+str(i+1) for i in range(num_principal_components)]\n",
    "\n",
    "#these are the weights calculated on the basis of molar weight of Hydrogen\n",
    "wopt = np.array([0.25131806468584, 1.0, 0.0, 0.0, 0.05926499970012948, 0.11189834407236524, 0.03053739933116691, 0.05926499970012948, 0.0, 0.07742283372149472, 0.14371856860332313, 0.14371856860332313, 0.20112514400193687, 1.0, 0.0, 0.0, 0.03473494419333629, 0.06713785861443991, 0.09743596683886535, 0.09743596683886535, 0.12582790137651187, 0.04027033873046593, 0.07742283372149472, 0.11180607885607882, 0.14371856860332313, 0.17341738612784788, 0.20112514400193687, 0.024566681794273966, 0.04795526192839207, 0.04795526192839207, 0.0, 0.06713048065088474, 0.12581494366075874, 0.17755300484072126, 0.034730994502665966, 0.0, 0.0, 0.0, 0.03249947443158002, 0.0, 0.0372961080230628, 0.07191024382448291, 0.024564706019978535, 0.023426986426879046, 0.023426986426879046, 0.023426986426879046, 0.0, 0.16374935944566987, 0.18286442054789118, 0.07024850027715426, 0.09152158240065958, 0.0, 0.0] , dtype=float)\n",
    "\n",
    "'''\n",
    "Zmix = wopt * Xi --> Zmix += mixFracMassCoeff[s] * Yi[s] in the following code and then it is normalized\n",
    "Z_f = 0.25131806468584\n",
    "Z_ox = 0.0\n",
    "(Zmix - Z_ox) / (Z_f - Z_ox)\n",
    "\n",
    "public double calc(double[] Yi) {\n",
    "\t\tdouble Zmix = 0.;\n",
    "\t\tfor (int s = 0; s < species.length; s++) {\n",
    "\t\t\tZmix += mixFracMassCoeff[s] * Yi[s];\n",
    "\t\t}\n",
    "\t\treturn (Zmix - Z_ox) / (Z_f - Z_ox);\n",
    "\t}\n",
    "'''\n",
    "\n",
    "# DD is this creating Zmix PCA data?\n",
    "w = wopt[:,np.newaxis]\n",
    "\n",
    "# center the data\n",
    "Xcenter = X - np.mean(X)\n",
    "\n",
    "A = np.cov(X.T)\n",
    "\n",
    "# calculate A - ww^TA\n",
    "L = A - np.dot(np.dot(w,w.T),A)\n",
    "\n",
    "# get the first eigen vector\n",
    "values,vectors = np.linalg.eig(L)\n",
    "\n",
    "vectors = np.real(vectors)\n",
    "\n",
    "values = np.real(values)\n",
    "\n",
    "df_zmix_pca = pd.DataFrame()\n",
    "\n",
    "'''\n",
    "To reproduce Zmix the actual formula should be \n",
    "\n",
    "df_zmix_pca[zmix_pca_dim_cols[0]] = X.dot(wopt)/0.25131806468584\n",
    "\n",
    "instead of\n",
    "\n",
    "df_zmix_pca[zmix_pca_dim_cols[0]] = Xcenter.dot(wopt)\n",
    "'''\n",
    "\n",
    "df_zmix_pca[zmix_pca_dim_cols[0]] = X.dot(wopt)/0.25131806468584\n",
    "\n",
    "for i in range(len(zmix_pca_dim_cols)-1):\n",
    "    df_zmix_pca[zmix_pca_dim_cols[i+1]] = Xcenter.dot(vectors.T[i])\n",
    "        \n",
    "df = pd.concat([df,df_zmix_pca], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SPARSE_PCA_1</th>\n",
       "      <td>16438.0</td>\n",
       "      <td>1.880314e-17</td>\n",
       "      <td>0.286244</td>\n",
       "      <td>-0.750169</td>\n",
       "      <td>-0.195068</td>\n",
       "      <td>0.150254</td>\n",
       "      <td>0.233590</td>\n",
       "      <td>0.233613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SPARSE_PCA_2</th>\n",
       "      <td>16438.0</td>\n",
       "      <td>-3.274341e-17</td>\n",
       "      <td>0.225461</td>\n",
       "      <td>-0.129801</td>\n",
       "      <td>-0.129194</td>\n",
       "      <td>-0.119788</td>\n",
       "      <td>0.014575</td>\n",
       "      <td>0.842072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SPARSE_PCA_3</th>\n",
       "      <td>16438.0</td>\n",
       "      <td>2.053217e-17</td>\n",
       "      <td>0.123735</td>\n",
       "      <td>-0.219611</td>\n",
       "      <td>-0.104978</td>\n",
       "      <td>0.005773</td>\n",
       "      <td>0.099920</td>\n",
       "      <td>0.256032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SPARSE_PCA_4</th>\n",
       "      <td>16438.0</td>\n",
       "      <td>5.349170e-18</td>\n",
       "      <td>0.104063</td>\n",
       "      <td>-0.141960</td>\n",
       "      <td>-0.092118</td>\n",
       "      <td>-0.016206</td>\n",
       "      <td>0.080881</td>\n",
       "      <td>0.231266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SPARSE_PCA_5</th>\n",
       "      <td>16438.0</td>\n",
       "      <td>-1.188705e-18</td>\n",
       "      <td>0.063358</td>\n",
       "      <td>-0.142198</td>\n",
       "      <td>-0.041493</td>\n",
       "      <td>0.009796</td>\n",
       "      <td>0.048435</td>\n",
       "      <td>0.107396</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                count          mean       std       min       25%       50%  \\\n",
       "SPARSE_PCA_1  16438.0  1.880314e-17  0.286244 -0.750169 -0.195068  0.150254   \n",
       "SPARSE_PCA_2  16438.0 -3.274341e-17  0.225461 -0.129801 -0.129194 -0.119788   \n",
       "SPARSE_PCA_3  16438.0  2.053217e-17  0.123735 -0.219611 -0.104978  0.005773   \n",
       "SPARSE_PCA_4  16438.0  5.349170e-18  0.104063 -0.141960 -0.092118 -0.016206   \n",
       "SPARSE_PCA_5  16438.0 -1.188705e-18  0.063358 -0.142198 -0.041493  0.009796   \n",
       "\n",
       "                   75%       max  \n",
       "SPARSE_PCA_1  0.233590  0.233613  \n",
       "SPARSE_PCA_2  0.014575  0.842072  \n",
       "SPARSE_PCA_3  0.099920  0.256032  \n",
       "SPARSE_PCA_4  0.080881  0.231266  \n",
       "SPARSE_PCA_5  0.048435  0.107396  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[sparse_pca_dim_cols].describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>PURE_PCA_1</th>\n",
       "      <td>16438.0</td>\n",
       "      <td>-4.970946e-18</td>\n",
       "      <td>0.325067</td>\n",
       "      <td>-0.491032</td>\n",
       "      <td>-0.237567</td>\n",
       "      <td>-0.114702</td>\n",
       "      <td>0.235078</td>\n",
       "      <td>0.791924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PURE_PCA_2</th>\n",
       "      <td>16438.0</td>\n",
       "      <td>-4.106434e-17</td>\n",
       "      <td>0.238644</td>\n",
       "      <td>-0.317777</td>\n",
       "      <td>-0.189794</td>\n",
       "      <td>-0.045926</td>\n",
       "      <td>0.133166</td>\n",
       "      <td>0.784243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PURE_PCA_3</th>\n",
       "      <td>16438.0</td>\n",
       "      <td>3.633654e-18</td>\n",
       "      <td>0.060422</td>\n",
       "      <td>-0.151623</td>\n",
       "      <td>-0.025278</td>\n",
       "      <td>0.003241</td>\n",
       "      <td>0.035352</td>\n",
       "      <td>0.181621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PURE_PCA_4</th>\n",
       "      <td>16438.0</td>\n",
       "      <td>-1.642574e-17</td>\n",
       "      <td>0.017966</td>\n",
       "      <td>-0.041385</td>\n",
       "      <td>-0.011443</td>\n",
       "      <td>-0.001986</td>\n",
       "      <td>0.007269</td>\n",
       "      <td>0.058252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PURE_PCA_5</th>\n",
       "      <td>16438.0</td>\n",
       "      <td>-1.431849e-18</td>\n",
       "      <td>0.012196</td>\n",
       "      <td>-0.026776</td>\n",
       "      <td>-0.008930</td>\n",
       "      <td>-0.000457</td>\n",
       "      <td>0.008278</td>\n",
       "      <td>0.028622</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              count          mean       std       min       25%       50%  \\\n",
       "PURE_PCA_1  16438.0 -4.970946e-18  0.325067 -0.491032 -0.237567 -0.114702   \n",
       "PURE_PCA_2  16438.0 -4.106434e-17  0.238644 -0.317777 -0.189794 -0.045926   \n",
       "PURE_PCA_3  16438.0  3.633654e-18  0.060422 -0.151623 -0.025278  0.003241   \n",
       "PURE_PCA_4  16438.0 -1.642574e-17  0.017966 -0.041385 -0.011443 -0.001986   \n",
       "PURE_PCA_5  16438.0 -1.431849e-18  0.012196 -0.026776 -0.008930 -0.000457   \n",
       "\n",
       "                 75%       max  \n",
       "PURE_PCA_1  0.235078  0.791924  \n",
       "PURE_PCA_2  0.133166  0.784243  \n",
       "PURE_PCA_3  0.035352  0.181621  \n",
       "PURE_PCA_4  0.007269  0.058252  \n",
       "PURE_PCA_5  0.008278  0.028622  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[pure_pca_dim_cols].describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Zmix_PCA_1</th>\n",
       "      <td>16438.0</td>\n",
       "      <td>0.429979</td>\n",
       "      <td>0.260281</td>\n",
       "      <td>0.004543</td>\n",
       "      <td>0.212661</td>\n",
       "      <td>0.396423</td>\n",
       "      <td>0.631239</td>\n",
       "      <td>0.995403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Zmix_PCA_2</th>\n",
       "      <td>16438.0</td>\n",
       "      <td>0.071971</td>\n",
       "      <td>0.309412</td>\n",
       "      <td>-0.387519</td>\n",
       "      <td>-0.153328</td>\n",
       "      <td>-0.041812</td>\n",
       "      <td>0.294393</td>\n",
       "      <td>0.830963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Zmix_PCA_3</th>\n",
       "      <td>16438.0</td>\n",
       "      <td>0.047207</td>\n",
       "      <td>0.238312</td>\n",
       "      <td>-0.740353</td>\n",
       "      <td>-0.083435</td>\n",
       "      <td>0.093493</td>\n",
       "      <td>0.235950</td>\n",
       "      <td>0.362902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Zmix_PCA_4</th>\n",
       "      <td>16438.0</td>\n",
       "      <td>0.077558</td>\n",
       "      <td>0.059723</td>\n",
       "      <td>-0.072785</td>\n",
       "      <td>0.052950</td>\n",
       "      <td>0.080639</td>\n",
       "      <td>0.112492</td>\n",
       "      <td>0.256763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Zmix_PCA_5</th>\n",
       "      <td>16438.0</td>\n",
       "      <td>-0.081251</td>\n",
       "      <td>0.017963</td>\n",
       "      <td>-0.139505</td>\n",
       "      <td>-0.088491</td>\n",
       "      <td>-0.079294</td>\n",
       "      <td>-0.069821</td>\n",
       "      <td>-0.039879</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              count      mean       std       min       25%       50%  \\\n",
       "Zmix_PCA_1  16438.0  0.429979  0.260281  0.004543  0.212661  0.396423   \n",
       "Zmix_PCA_2  16438.0  0.071971  0.309412 -0.387519 -0.153328 -0.041812   \n",
       "Zmix_PCA_3  16438.0  0.047207  0.238312 -0.740353 -0.083435  0.093493   \n",
       "Zmix_PCA_4  16438.0  0.077558  0.059723 -0.072785  0.052950  0.080639   \n",
       "Zmix_PCA_5  16438.0 -0.081251  0.017963 -0.139505 -0.088491 -0.079294   \n",
       "\n",
       "                 75%       max  \n",
       "Zmix_PCA_1  0.631239  0.995403  \n",
       "Zmix_PCA_2  0.294393  0.830963  \n",
       "Zmix_PCA_3  0.235950  0.362902  \n",
       "Zmix_PCA_4  0.112492  0.256763  \n",
       "Zmix_PCA_5 -0.069821 -0.039879  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[zmix_pca_dim_cols].describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Zmix</th>\n",
       "      <td>16438.0</td>\n",
       "      <td>0.429979</td>\n",
       "      <td>0.260281</td>\n",
       "      <td>0.004543</td>\n",
       "      <td>0.212661</td>\n",
       "      <td>0.396423</td>\n",
       "      <td>0.631239</td>\n",
       "      <td>0.995403</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        count      mean       std       min       25%       50%       75%  \\\n",
       "Zmix  16438.0  0.429979  0.260281  0.004543  0.212661  0.396423  0.631239   \n",
       "\n",
       "           max  \n",
       "Zmix  0.995403  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['Zmix']].describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparing the 'X' matrix column arrangement for the Constrained PCA formulation\n",
    "#icovariates\n",
    "#[CH4, H, O, O2, OH, H2O, HO2, H2O2, C, CH, CH2, CH2(S), CH3, H2, CO, CO2, HCO, CH2O, CH2OH, CH3O, CH3OH, C2H, C2H2, C2H3, C2H4, C2H5, C2H6, HCCO, CH2CO, HCCOH, N, NH, NH2, NH3, NNH, NO, NO2, N2O, HNO, CN, HCN, H2CN, HCNN, HCNO, HOCN, HNCO, NCO, C3H7, C3H8, CH2CHO, CH3CHO, N2, AR]\n",
    "#df.head()['Zmix']\n",
    "#X[3].dot(wopt)/0.25131806468584\n",
    "#Zmix matches with the calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport pandas as pd\\nimport numpy as np\\nimport sklearn\\n\\ndates=['April-10', 'April-11', 'April-12', 'April-13']\\nfruits=['Apple', 'Papaya', 'Banana', 'Mango']\\nprices=[3, 1, 2, 4]\\n\\n\\n\\ntdf = pd.DataFrame({'Date':dates ,\\n                   'Fruit':fruits ,\\n                   'Price': prices})\\n\\nprint('Before')\\nprint(tdf)\\n\\ntdf_shuffled=sklearn.utils.shuffle(tdf,random_state=0)\\nprint(tdf_shuffled)\\nprint('After')\\nprint(tdf)\\n\\n\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "dates=['April-10', 'April-11', 'April-12', 'April-13']\n",
    "fruits=['Apple', 'Papaya', 'Banana', 'Mango']\n",
    "prices=[3, 1, 2, 4]\n",
    "\n",
    "\n",
    "\n",
    "tdf = pd.DataFrame({'Date':dates ,\n",
    "                   'Fruit':fruits ,\n",
    "                   'Price': prices})\n",
    "\n",
    "print('Before')\n",
    "print(tdf)\n",
    "\n",
    "tdf_shuffled=sklearn.utils.shuffle(tdf,random_state=0)\n",
    "print(tdf_shuffled)\n",
    "print('After')\n",
    "print(tdf)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "framework_included_flames_int = df[df['is_flame_included_by_framework'] == 1]['flame_key_int'].unique()\n",
    "\n",
    "framework_excluded_flames_int = df[df['is_flame_included_by_framework'] == 0]['flame_key_int'].unique()\n",
    "\n",
    "all_flames_int = df['flame_key_int'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unmatched ')' (<ipython-input-5-414b24750fa1>, line 47)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-414b24750fa1>\"\u001b[0;36m, line \u001b[0;32m47\u001b[0m\n\u001b[0;31m    Y_df = pd.DataFrame(data=Y_array, columns=self.output_data_cols))\u001b[0m\n\u001b[0m                                                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unmatched ')'\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Aug  4 19:42:05 2021\n",
    "\n",
    "@author: amol\n",
    "\"\"\"\n",
    "import os\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib\n",
    "matplotlib.use('tAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class DataManager:\n",
    "    def __init__(self, df_totalData, constants):\n",
    "        self.constants = constants\n",
    "        self.df = df_totalData\n",
    "        self.outputScaler = None\n",
    "        self.inputScaler = None\n",
    "        self.zmixScaler = None\n",
    "        self.df_training = None\n",
    "        self.df_testing = None\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.X_scaled_train = None\n",
    "        self.X_scaled_test = None\n",
    "        self.rom_train = None\n",
    "        self.rom_test = None\n",
    "        self.rom_scaled_train = None\n",
    "        self.rom_scaled_test = None\n",
    "        self.zmix_train = None\n",
    "        self.zmix_test = None\n",
    "        self.zmix_scaled_train = None\n",
    "        self.zmix_scaled_test = None\n",
    "        self.Y_train = None\n",
    "        self.Y_test = None\n",
    "        self.Y_scaled_train = None\n",
    "        self.Y_scaled_test = None\n",
    "        self.input_data_cols = None\n",
    "        self.output_data_cols = None\n",
    "        \n",
    "        return\n",
    "\n",
    "    # does permutation feature importance test on an arbitrary model & reports graphically/textually the results\n",
    "    def do_perm_feature_importance(self, model, model_name='Model', random_state=0):\n",
    "        Y_df = pd.DataFrame(data=Y_array, columns=self.output_data_cols))\n",
    "        X_df = pd.DataFrame(data=X_array, columns=self.input_data_cols))\n",
    "\n",
    "        from sklearn.inspection import permutation_importance\n",
    "        r = permutation_importance(model, X_data, Y_data,\n",
    "                                   n_repeats=30, random_state=random_state)\n",
    "\"~/rom_project/src/data/train_test_manager.py\" [dos] 245L, 10602C                                                                                                                         1,1           Top\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = DataManager(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Process Ablation Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomGPR(GaussianProcessRegressor):\n",
    "    def __init__(self, *args, max_iter=150,max_fun=50, gtol=1e-05, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self._max_iter = max_iter\n",
    "        self._gtol = gtol\n",
    "        self._max_fun = max_fun\n",
    "\n",
    "    #ftol = 1000000000000.0 (factr) * np.finfo(float).eps --> 0.0002220446049250313\n",
    "    #,'ftol':0.0000002220446049250313\n",
    "    def _constrained_optimization(self, obj_func, initial_theta, bounds):\n",
    "        if self.optimizer == \"fmin_l_bfgs_b\":\n",
    "            opt_res = scipy.optimize.minimize(obj_func, initial_theta, method=\"L-BFGS-B\", jac=True, bounds=bounds, options={'maxiter':self._max_iter,'maxfun':self._max_fun,'gtol': self._gtol})\n",
    "            _check_optimize_result(\"lbfgs\", opt_res)\n",
    "            theta_opt, func_min = opt_res.x, opt_res.fun\n",
    "        elif callable(self.optimizer):\n",
    "            theta_opt, func_min = self.optimizer(obj_func, initial_theta, bounds=bounds)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown optimizer %s.\" % self.optimizer)\n",
    "        return theta_opt, func_min\n",
    "\n",
    "\n",
    "\n",
    "def getGPModel(kernel=\"Matern\"):\n",
    "    if kernel == \"Matern_RationalQuadratic\":\n",
    "\n",
    "        # medium term irregularities\n",
    "        k1 = 0.5* Matern(length_scale=2, nu=3/2)\n",
    "        k2 = 0.5* RationalQuadratic(length_scale=1.0, alpha=1.0)\n",
    "\n",
    "        '''\n",
    "        k4 = 0.1**2 * RBF(length_scale=0.1) \\+ WhiteKernel(noise_level=0.1**2,\n",
    "                      noise_level_bounds=(1e-3, np.inf))  # noise terms\n",
    "        '''\n",
    "        kernel = k1 + k2\n",
    "\n",
    "    elif kernel == \"RationalQuadratic\":\n",
    "\n",
    "        # medium term irregularities\n",
    "        kernel = RationalQuadratic(length_scale=1.0, alpha=1.0)    \n",
    "\n",
    "    else:\n",
    "        kernel = Matern(length_scale=2, nu=3/2)\n",
    "\n",
    "    return CustomGPR(kernel=kernel) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''experimentTrackingFields = ['Model','Dataset','Cpv Type','#Cpv',\"ZmixExists\",'MAE','TAE','MSE','TSE','#Pts','FitTime','PredTime']\n",
    "\n",
    "df_experimentTracker = pd.DataFrame(columns=experimentTrackingFields)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport os#must import this library\\n\\nif os.path.exists(\\'results.csv\\'):\\n        os.remove(\\'results.csv\\') #this deletes the file\\nelse:\\n        print(\"The results.csv file does not exist\")\\n\\nresultsFile = open(\\'results.csv\\', \\'w\\')\\n\\nprintStr = \"\\t\"\\n\\nprintStr = printStr.join(experimentTrackingFields)\\n\\n\\nresultsFile.write(printStr)\\n\\nresultsFile.write(\"\\n\")\\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import os#must import this library\n",
    "\n",
    "if os.path.exists('results.csv'):\n",
    "        os.remove('results.csv') #this deletes the file\n",
    "else:\n",
    "        print(\"The results.csv file does not exist\")\n",
    "\n",
    "resultsFile = open('results.csv', 'w')\n",
    "\n",
    "printStr = \"\\t\"\n",
    "\n",
    "printStr = printStr.join(experimentTrackingFields)\n",
    "\n",
    "\n",
    "resultsFile.write(printStr)\n",
    "\n",
    "resultsFile.write(\"\\n\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentExecutor:\n",
    "    def __init__(self):\n",
    "        self.dm = None\n",
    "        self.modelType = None\n",
    "        self.model = None\n",
    "        self.df_experimentTracker = None\n",
    "        self.fit_time = None\n",
    "        self.pred_time = None\n",
    "        self.err = None\n",
    "     \n",
    "    def setModel(self,model):\n",
    "        self.model = model\n",
    "\n",
    "    def executeExperiment(self, dataManager, modelType, dataType=\"randomequaltraintestsplit\",inputType=\"ZmixPCA\",noOfCpv=5):\n",
    "        self.dm = dataManager\n",
    "        \n",
    "        self.modelType = modelType\n",
    "        \n",
    "        dataSetMethod = inputType + '_' + dataType\n",
    "        \n",
    "        self.dm.createTrainTestData(dataSetMethod,noOfCpv, None, None)\n",
    "        \n",
    "        self.fitModelAndCalcErr(self.dm.X_train, self.dm.Y_train, self.dm.X_test, self.dm.Y_test)\n",
    "        \n",
    "        if inputType.find('Zmix') != -1:\n",
    "            ZmixPresent = 'Y'\n",
    "        else:\n",
    "            ZmixPresent = 'N'\n",
    "\n",
    "        experimentResults = [self.modelType, dataType,inputType,str(noOfCpv), ZmixPresent,str(self.err[2]),str(self.err[0]),str(self.err[3]),str(self.err[1]),str(self.err[5]),str(self.fit_time),str(self.pred_time)]\n",
    "\n",
    "        printStr = \"self.modelType: \"+ self.modelType+ \" dataType: \"  + dataType+ \" inputType:\"+inputType+ \" noOfCpv:\"+str(noOfCpv)+ \" ZmixPresent:\" + ZmixPresent + \" MAE:\" +str(self.err[2])\n",
    "\n",
    "        print(printStr)\n",
    "        \n",
    "    def executeExperiments(self, dataManager, modelType, df_experimentTracker):\n",
    "        self.dm = dataManager\n",
    "        self.modelType = modelType\n",
    "        self.df_experimentTracker = df_experimentTracker\n",
    "        \n",
    "        #Experiments  \n",
    "        \n",
    "        dataTypes = [\"randomequaltraintestsplit\",\"frameworkincludedtrainexcludedtest\"]\n",
    "        #dataTypes = [\"frameworkincludedtrainexcludedtest\"]\n",
    "        \n",
    "        #inputTypes = [\"ZmixCpv\",\"ZmixPCA\",\"SparsePCA\",\"PurePCA\",\"ZmixAndPurePCA\",\"ZmixAndSparsePCA\",\"ZmixAllSpecies\",\"AllSpecies\"]\n",
    "        inputTypes = [\"ZmixAndPurePCA\",\"ZmixAndSparsePCA\"]\n",
    "        #inputTypes = [\"ZmixCpv\"]\n",
    "        \n",
    "        models = []\n",
    "        \n",
    "        for dataType in dataTypes:\n",
    "            \n",
    "            for inputType in inputTypes:\n",
    "                #ZmixCpv_randomequaltraintestsplit\n",
    "                dataSetMethod = inputType + '_' + dataType\n",
    "                        \n",
    "                if inputType.find('Zmix') != -1:\n",
    "                    ZmixPresent = 'Y'\n",
    "                else:\n",
    "                    ZmixPresent = 'N'\n",
    "                    \n",
    "                if inputType.find('PCA') != -1:\n",
    "                    \n",
    "                    noOfCpvs = [item for item in range(1, 6)]\n",
    "                    \n",
    "                    for noOfCpv in noOfCpvs:\n",
    "                        #                           dataSetMethod,noOfCpvs, ipscaler, opscaler\n",
    "                        self.dm.createTrainTestData(dataSetMethod,noOfCpv, None, None)\n",
    "                        self.fitModelAndCalcErr(self.dm.X_train, self.dm.Y_train, self.dm.X_test, self.dm.Y_test)\n",
    "                        experimentResults = [self.modelType, dataType,inputType,str(noOfCpv), ZmixPresent,str(self.err[2]),str(self.err[0]),str(self.err[3]),str(self.err[1]),str(self.err[5]),str(self.fit_time),str(self.pred_time)]\n",
    "                        self.df_experimentTracker.loc[len(self.df_experimentTracker)] = experimentResults        \n",
    "                        printStr = \"self.modelType: \"+ self.modelType+ \" dataType: \"  + dataType+ \" inputType:\"+inputType+ \" noOfCpv:\"+str(noOfCpv)+ \" ZmixPresent:\" + ZmixPresent + \" MAE:\" +str(self.err[2])\n",
    "                        print(printStr)\n",
    "                        printStr = \"\\t\"\n",
    "                        printStr = printStr.join(experimentResults)\n",
    "                        resultsFile.write(printStr)\n",
    "                        resultsFile.write(\"\\n\")\n",
    "                else:\n",
    "                   \n",
    "                    if inputType.find('ZmixCpv') != -1:\n",
    "                        noOfCpv = 1\n",
    "                    else:\n",
    "                        noOfCpv = 53\n",
    "                        \n",
    "                    #                           dataSetMethod,noOfCpvs, ipscaler, opscaler\n",
    "                    self.dm.createTrainTestData(dataSetMethod,noOfCpv, None, None)\n",
    "\n",
    "                    self.fitModelAndCalcErr(self.dm.X_train, self.dm.Y_train, self.dm.X_test, self.dm.Y_test)    \n",
    "\n",
    "                    experimentResults = [self.modelType, dataType,inputType,str(noOfCpv), ZmixPresent,str(self.err[2]),str(self.err[0]),str(self.err[3]),str(self.err[1]),str(self.err[5]),str(self.fit_time),str(self.pred_time)]\n",
    "                        \n",
    "                    self.df_experimentTracker.loc[len(self.df_experimentTracker)] = experimentResults        \n",
    "\n",
    "                    printStr = \"self.modelType: \"+ self.modelType+ \" dataType: \"  + dataType+ \" inputType:\"+inputType+ \" noOfCpv:\"+str(noOfCpv)+ \" ZmixPresent:\" + ZmixPresent + \" MAE:\" +str(self.err[2])\n",
    "\n",
    "                    print(printStr)\n",
    "\n",
    "                    printStr = \"\\t\"\n",
    "\n",
    "                    printStr = printStr.join(experimentResults)\n",
    "\n",
    "                    resultsFile.write(printStr)\n",
    "\n",
    "                    resultsFile.write(\"\\n\")\n",
    "        \n",
    "    def computeError (self, Y_pred, Y_test):\n",
    "        evaluation_df_1 = pd.DataFrame()\n",
    "\n",
    "        evaluation_df_1['souener'] = Y_test.flatten()\n",
    "\n",
    "        evaluation_df_1['souener_pred'] = Y_pred.flatten()\n",
    "\n",
    "        evaluation_df_1['souener_pred_L1'] = evaluation_df_1['souener'] - evaluation_df_1['souener_pred'] \n",
    "\n",
    "        evaluation_df_1['souener_pred_L2'] = evaluation_df_1['souener_pred_L1'] * evaluation_df_1['souener_pred_L1']\n",
    "\n",
    "        evaluation_df_1['souener_pred_L1Percent'] = ((evaluation_df_1['souener'] - evaluation_df_1['souener_pred'])/evaluation_df_1['souener']) \n",
    "\n",
    "        TotalAbsoluteError = evaluation_df_1['souener_pred_L1'].abs().sum()\n",
    "\n",
    "        TotalSquaredError = evaluation_df_1['souener_pred_L2'].abs().sum()\n",
    "\n",
    "        MeanAbsoluteError = evaluation_df_1['souener_pred_L1'].abs().sum()/evaluation_df_1['souener_pred_L1'].abs().count()\n",
    "\n",
    "        MeanSquaredError = evaluation_df_1['souener_pred_L2'].abs().sum()/evaluation_df_1['souener_pred_L2'].abs().count()\n",
    "\n",
    "        NumPoints = evaluation_df_1['souener_pred_L1Percent'].abs().count()\n",
    "\n",
    "        MeanPercentageError = evaluation_df_1['souener_pred_L1Percent'].abs().sum()/NumPoints\n",
    "\n",
    "        return [TotalAbsoluteError,TotalSquaredError,MeanAbsoluteError,MeanSquaredError,MeanPercentageError,NumPoints]\n",
    "\n",
    "            \n",
    "    def printError (self,err):\n",
    "        TotalAbsoluteError = err[0]\n",
    "\n",
    "        TotalSquaredError = err[1]\n",
    "\n",
    "        MeanAbsoluteError = err[2]\n",
    "\n",
    "        MeanSquaredError = err[3]\n",
    "\n",
    "        MeanPercentageError = err[4]\n",
    "\n",
    "        NumPoints = err[5]\n",
    "        \n",
    "        print ('Total Absolute Error: ', TotalAbsoluteError)\n",
    "        print ('Mean Absolute Error: ', MeanAbsoluteError)\n",
    "        print ('Mean Percentage Error: ', MeanPercentageError)\n",
    "        print ('Total Squared Error: ', TotalSquaredError)\n",
    "        print ('Mean Squared Error: ', MeanSquaredError)\n",
    "        print ('Number of Points: ', NumPoints)\n",
    "\n",
    "    def computeAndPrintError(self,Y_pred, Y_test):    \n",
    "        self.err = computeError (Y_pred, Y_test)\n",
    "        printError (err)\n",
    "        logError(err)\n",
    "        return err\n",
    "        \n",
    "    def fitModelAndCalcErr(self, X_train, Y_train, X_test, Y_test):\n",
    "\n",
    "        t = time.process_time()\n",
    "\n",
    "        self.model.fit(X_train, Y_train)\n",
    "\n",
    "        self.fit_time = time.process_time() - t\n",
    "\n",
    "        t = time.process_time()\n",
    "\n",
    "        Y_pred = self.model.predict(X_test, return_std=False)\n",
    "\n",
    "        self.pred_time = time.process_time() - t\n",
    "        \n",
    "        #computeAndPrintError(Y_pred, Y_test)\n",
    "\n",
    "        self.err = self.computeError (Y_pred, Y_test)\n",
    "        \n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In _createTrainTestData Y_test.shape: (8219, 1)\n",
      "In createTrainTestData Y_test.shape: (8219, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-104-8d5b26e3952b>:13: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of f AND g EVALUATIONS EXCEEDS LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  _check_optimize_result(\"lbfgs\", opt_res)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.modelType: GP_Matern dataType: randomequaltraintestsplit inputType:ZmixPCA noOfCpv:5 ZmixPresent:Y MAE:695308313.75533\n"
     ]
    }
   ],
   "source": [
    "expExectr = ExperimentExecutor()\n",
    "expExectr.setModel(getGPModel(\"Matern\"))\n",
    "expExectr.executeExperiment(dm, \"GP_Matern\", dataType=\"randomequaltraintestsplit\",inputType=\"ZmixPCA\",noOfCpv=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-0535c8d4755b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdo_perm_feature_importance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpExectr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpExectr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpExectr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-100-75787fa4e7ef>\u001b[0m in \u001b[0;36mdo_perm_feature_importance\u001b[0;34m(model, X_data, Y_data, random_state)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margsort\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m          \u001b[0;32mif\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimportances_mean\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimportances_std\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{list(X_data.columns)[i]}; {r.importances_mean[i]:.3f} +/- {r.importances_std[i]:.3f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimportances_mean\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "do_perm_feature_importance(expExectr.model, expExectr.dm.X_test, expExectr.dm.Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "expExectr = ExperimentExecutor()\n",
    "expExectr.setModel(getGPModel(\"Matern\"))\n",
    "expExectr.executeExperiments(dm, \"GP_Matern\", df_experimentTracker)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "expExectr = ExperimentExecutor()\n",
    "expExectr.setModel(getGPModel(\"Matern_RationalQuadratic\"))\n",
    "expExectr.executeExperiments(dm, \"GP_Matern_RationalQuadratic\", df_experimentTracker)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "expExectr = ExperimentExecutor()\n",
    "expExectr.setModel(getGPModel(\"RationalQuadratic\"))\n",
    "expExectr.executeExperiments(dm, \"GP_RationalQuadratic\", df_experimentTracker)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_experimentTracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "df_experimentTracker.to_csv('PAPER1_PCA_GP_DNN_NEIGHBORDNN_PCDNN_V2_Experiments.csv', sep='\\t',encoding='utf-8', index=False)\n",
    "\n",
    "#!!!!!!! Very Important !!!!!!\n",
    "resultsFile.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple DNN Ablation Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getResiduals(Y_test,Y_pred):\n",
    "    evaluation_df_1 = pd.DataFrame()\n",
    "\n",
    "    evaluation_df_1['souener'] = Y_test.flatten()\n",
    "\n",
    "    evaluation_df_1['souener_pred'] = Y_pred.flatten()\n",
    "\n",
    "    evaluation_df_1['res'] = evaluation_df_1['souener'] - evaluation_df_1['souener_pred'] \n",
    "   \n",
    "    return evaluation_df_1['res'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history):\n",
    "    plt.plot(history.history['loss'], label='loss')\n",
    "    plt.plot(history.history['val_loss'], label='val_loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Error [Souener]')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnnexperimentTrackingFields = ['Model','Dataset','Cpv Type','#Cpv',\"ZmixExists\",'MAE','TAE','MSE','TSE','#Pts','FitTime','PredTime','MAX-MAE','MAX-TAE','MAX-MSE','MAX-TSE','MIN-MAE','MIN-TAE','MIN-MSE','MIN-TSE']\n",
    "df_dnnexperimentTracker = pd.DataFrame(columns=dnnexperimentTrackingFields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import os#must import this library\n",
    "\n",
    "if os.path.exists('results.csv'):\n",
    "        os.remove('results.csv') #this deletes the file\n",
    "else:\n",
    "        print(\"The results.csv file does not exist\")\n",
    "\n",
    "resultsFile = open('results.csv', 'w')\n",
    "\n",
    "printStr = \"\\t\"\n",
    "\n",
    "printStr = printStr.join(dnnexperimentTrackingFields)\n",
    "\n",
    "\n",
    "resultsFile.write(printStr)\n",
    "\n",
    "resultsFile.write(\"\\n\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "# patient early stopping\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=45)\n",
    "\n",
    "class DNNExperimentExecutor:\n",
    "    def __init__(self):\n",
    "        self.dm = None\n",
    "        self.modelType = None\n",
    "        self.model = None\n",
    "        self.df_experimentTracker = None\n",
    "        self.fit_time = None\n",
    "        self.pred_time = None\n",
    "        self.err = None\n",
    "        self.df_err = None \n",
    "        self.width = 512\n",
    "        self.halfwidth = 128    \n",
    "    \n",
    "    def setModel(self,model):\n",
    "        self.model = model\n",
    "        \n",
    "\n",
    "\n",
    "    def build_and_compile_simple_dnn_model(self,noOfInputNeurons):\n",
    "        inputs = keras.Input(shape=(noOfInputNeurons,), name=\"inputs\")\n",
    "\n",
    "        x = layers.Dense(32, activation=\"relu\")(inputs)\n",
    "        x = layers.Dense(64, activation=\"relu\")(x)\n",
    "        x = layers.Dense(128, activation=\"relu\")(x)\n",
    "        x = layers.Dense(256, activation=\"relu\")(x)\n",
    "        x = layers.Dense(512, activation=\"relu\")(x)\n",
    "        x = layers.Dense(256, activation=\"relu\")(x)\n",
    "        x = layers.Dense(128, activation=\"relu\")(x)\n",
    "        x = layers.Dense(64, activation=\"relu\")(x)\n",
    "        x = layers.Dense(32, activation=\"relu\")(x)\n",
    "\n",
    "        \n",
    "        souener_pred = layers.Dense(1, name=\"prediction\")(x)\n",
    "\n",
    "        model = keras.Model(\n",
    "            inputs=[inputs],\n",
    "            outputs=[souener_pred],\n",
    "        )\n",
    "\n",
    "        opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "        \n",
    "        model.compile(loss='mean_absolute_error',optimizer=opt)\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def executeExperiment(self, dataManager, modelType, dataType=\"randomequaltraintestsplit\", inputType=\"ZmixPCA\",noOfCpv=5):\n",
    "        self.dm = dataManager\n",
    "        \n",
    "        self.modelType = modelType\n",
    "        \n",
    "        dataSetMethod = inputType + '_' + dataType\n",
    "        \n",
    "        self.dm.createTrainTestData(dataSetMethod,noOfCpv, None, None)\n",
    "        \n",
    "        self.fitModelAndCalcErr(self.dm.X_train, self.dm.Y_train, self.dm.X_test, self.dm.Y_test)\n",
    "        \n",
    "        if inputType.find('Zmix') != -1:\n",
    "            ZmixPresent = 'Y'\n",
    "        else:\n",
    "            ZmixPresent = 'N'\n",
    "\n",
    "                            #['Model','Dataset','Cpv Type','#Cpv',\"ZmixExists\",'MAE','TAE','MSE','TSE','#Pts','FitTime','PredTime','MAX-MAE','MAX-TAE','MAX-MSE','MAX-TSE','MIN-MAE','MIN-TAE','MIN-MSE','MIN-TSE']\n",
    "\n",
    "        experimentResults = [self.modelType, dataType,inputType,str(noOfCpv), ZmixPresent,str(self.df_err['MAE'].mean()),str(self.df_err['TAE'].mean()),str(self.df_err['MSE'].mean()),str(self.df_err['TSE'].mean()),str(self.df_err['#Pts'].mean()),str(self.fit_time),str(self.pred_time),str(self.df_err['MAE'].max()),str(self.df_err['TAE'].max()),str(self.df_err['MSE'].max()),str(self.df_err['TSE'].max()),str(self.df_err['MAE'].min()),str(self.df_err['TAE'].min()),str(self.df_err['MSE'].min()),str(self.df_err['TSE'].min())]\n",
    "\n",
    "        printStr = \"self.modelType: \"+ self.modelType+ \" dataType: \"  + dataType+ \" inputType:\"+inputType+ \" noOfCpv:\"+str(noOfCpv)+ \" ZmixPresent:\" + ZmixPresent + \" MAE:\" +str(self.err[2])\n",
    "\n",
    "        print(printStr)\n",
    "    \n",
    "    def executeSingleExperiment(self,noOfInputNeurons,dataSetMethod,dataType,inputType,ZmixPresent,noOfCpv):\n",
    "        \n",
    "        print('------------------ ' + 'executeSingleExperiments' + ' ------------------')\n",
    "        \n",
    "        #                           dataSetMethod,noOfCpvs, ipscaler, opscaler\n",
    "        self.dm.createTrainTestData(dataSetMethod,noOfCpv, None, None)\n",
    "        \n",
    "        self.model = self.build_and_compile_simple_dnn_model(noOfInputNeurons)\n",
    "        \n",
    "        self.model.summary()\n",
    "\n",
    "        self.fitModelAndCalcErr(self.dm.X_train, self.dm.Y_train, self.dm.X_test, self.dm.Y_test)\n",
    "\n",
    "         #['Model','Dataset','Cpv Type','#Cpv',\"ZmixExists\",'MAE','TAE','MSE','TSE','#Pts','FitTime','PredTime','MAX-MAE','MAX-TAE','MAX-MSE','MAX-TSE','MIN-MAE','MIN-TAE','MIN-MSE','MIN-TSE']\n",
    "\n",
    "        experimentResults = [self.modelType, dataType,inputType,str(noOfCpv), ZmixPresent,str(self.df_err['MAE'].mean()),str(self.df_err['TAE'].mean()),str(self.df_err['MSE'].mean()),str(self.df_err['TSE'].mean()),str(self.df_err['#Pts'].mean()),str(self.fit_time),str(self.pred_time),str(self.df_err['MAE'].max()),str(self.df_err['TAE'].max()),str(self.df_err['MSE'].max()),str(self.df_err['TSE'].max()),str(self.df_err['MAE'].min()),str(self.df_err['TAE'].min()),str(self.df_err['MSE'].min()),str(self.df_err['TSE'].min())]\n",
    "\n",
    "        self.df_experimentTracker.loc[len(self.df_experimentTracker)] = experimentResults        \n",
    "\n",
    "        printStr = \"self.modelType: \"+ self.modelType+ \" dataType: \"  + dataType+ \" inputType:\"+inputType+ \" noOfCpv:\"+str(noOfCpv)+ \" ZmixPresent:\" + ZmixPresent + \" MAE:\" +str(self.df_err['MAE'].min())\n",
    "\n",
    "        print(printStr)\n",
    "\n",
    "        printStr = \"\\t\"\n",
    "\n",
    "        printStr = printStr.join(experimentResults)\n",
    "\n",
    "        resultsFile.write(printStr)\n",
    "\n",
    "        resultsFile.write(\"\\n\")\n",
    "\n",
    "    def executeExperiments(self,dataManager, modelType, df_experimentTracker):\n",
    "        self.dm = dataManager\n",
    "        self.modelType = modelType\n",
    "        self.df_experimentTracker = df_experimentTracker\n",
    "        \n",
    "        #Experiments  \n",
    "        \n",
    "        dataTypes = [\"randomequaltraintestsplit\",\"frameworkincludedtrainexcludedtest\"]\n",
    "        #dataTypes = [\"frameworkincludedtrainexcludedtest\"]\n",
    "        #dataTypes = [\"randomequaltraintestsplit\"]\n",
    "        \n",
    "        inputTypes = [\"ZmixCpv\",\"ZmixPCA\",\"SparsePCA\",\"PurePCA\",\"ZmixAndPurePCA\",\"ZmixAndSparsePCA\",\"ZmixAllSpecies\",\"AllSpecies\"]\n",
    "        #inputTypes = [\"ZmixAndPurePCA\",\"ZmixAndSparsePCA\"]\n",
    "        #inputTypes = [\"ZmixCpv\"]\n",
    "        #inputTypes = [\"PurePCA\"]\n",
    "        \n",
    "        for dataType in dataTypes:\n",
    "            print('=================== ' + dataType + ' ===================')\n",
    "            \n",
    "            for inputType in inputTypes:\n",
    "                \n",
    "                print('------------------ ' + inputType + ' ------------------')\n",
    "                    \n",
    "                #ZmixCpv_randomequaltraintestsplit\n",
    "                dataSetMethod = inputType + '_' + dataType\n",
    "                \n",
    "                noOfNeurons = 0\n",
    "                #ZmixAnd & ZmixAll\n",
    "                if inputType.find('ZmixA') != -1:\n",
    "                    noOfNeurons = noOfNeurons + 1\n",
    "\n",
    "                if inputType.find('Zmix') != -1:\n",
    "                    ZmixPresent = 'Y'\n",
    "                else:\n",
    "                    ZmixPresent = 'N'\n",
    "                    \n",
    "                if inputType.find('PCA') != -1:\n",
    "                    \n",
    "                    noOfCpvs = [item for item in range(1, 6)]\n",
    "                    \n",
    "                    for noOfCpv in noOfCpvs:\n",
    "                        noOfNeurons = noOfNeurons + 1                        \n",
    "                        self.executeSingleExperiment(noOfNeurons,dataSetMethod,dataType,inputType,ZmixPresent,noOfCpv)\n",
    "                else:\n",
    "\n",
    "                    if inputType.find('ZmixCpv') != -1:\n",
    "                        noOfCpv = 1\n",
    "                        noOfNeurons = 2\n",
    "                    else:\n",
    "                        noOfCpv = 53\n",
    "                        noOfNeurons = noOfNeurons + noOfCpv\n",
    "                    print('------------------ ' + str(noOfNeurons) + ' ------------------')\n",
    "                    self.executeSingleExperiment(noOfNeurons,dataSetMethod,dataType,inputType,ZmixPresent,noOfCpv)\n",
    "        \n",
    "    def computeError (self,Y_pred, Y_test):\n",
    "        evaluation_df_1 = pd.DataFrame()\n",
    "\n",
    "        evaluation_df_1['souener'] = Y_test.flatten()\n",
    "\n",
    "        evaluation_df_1['souener_pred'] = Y_pred.flatten()\n",
    "\n",
    "        evaluation_df_1['souener_pred_L1'] = evaluation_df_1['souener'] - evaluation_df_1['souener_pred'] \n",
    "\n",
    "        evaluation_df_1['souener_pred_L2'] = evaluation_df_1['souener_pred_L1'] * evaluation_df_1['souener_pred_L1']\n",
    "\n",
    "        evaluation_df_1['souener_pred_L1Percent'] = ((evaluation_df_1['souener'] - evaluation_df_1['souener_pred'])/evaluation_df_1['souener']) \n",
    "\n",
    "        TotalAbsoluteError = evaluation_df_1['souener_pred_L1'].abs().sum()\n",
    "\n",
    "        TotalSquaredError = evaluation_df_1['souener_pred_L2'].abs().sum()\n",
    "\n",
    "        MeanAbsoluteError = evaluation_df_1['souener_pred_L1'].abs().sum()/evaluation_df_1['souener_pred_L1'].abs().count()\n",
    "\n",
    "        MeanSquaredError = evaluation_df_1['souener_pred_L2'].abs().sum()/evaluation_df_1['souener_pred_L2'].abs().count()\n",
    "\n",
    "        NumPoints = evaluation_df_1['souener_pred_L1Percent'].abs().count()\n",
    "\n",
    "        MeanPercentageError = evaluation_df_1['souener_pred_L1Percent'].abs().sum()/NumPoints\n",
    "\n",
    "        return [TotalAbsoluteError,TotalSquaredError,MeanAbsoluteError,MeanSquaredError,MeanPercentageError,NumPoints]\n",
    "\n",
    "            \n",
    "    def printError (self,err):\n",
    "        TotalAbsoluteError = err[0]\n",
    "\n",
    "        TotalSquaredError = err[1]\n",
    "\n",
    "        MeanAbsoluteError = err[2]\n",
    "\n",
    "        MeanSquaredError = err[3]\n",
    "\n",
    "        MeanPercentageError = err[4]\n",
    "\n",
    "        NumPoints = err[5]\n",
    "        \n",
    "        print ('Total Absolute Error: ', TotalAbsoluteError)\n",
    "        print ('Mean Absolute Error: ', MeanAbsoluteError)\n",
    "        print ('Mean Percentage Error: ', MeanPercentageError)\n",
    "        print ('Total Squared Error: ', TotalSquaredError)\n",
    "        print ('Mean Squared Error: ', MeanSquaredError)\n",
    "        print ('Number of Points: ', NumPoints)\n",
    "\n",
    "    def computeAndPrintError(self,Y_pred, Y_test):    \n",
    "        self.err = computeError (Y_pred, Y_test)\n",
    "        printError (err)\n",
    "        logError(err)\n",
    "        return err    \n",
    "        \n",
    "        \n",
    "        \n",
    "    def fitModelAndCalcErr(self,X_train, Y_train, X_test, Y_test):\n",
    "\n",
    "        fit_times = []\n",
    "        \n",
    "        pred_times = []\n",
    "        \n",
    "        errs = []\n",
    "        \n",
    "        for itr in range(1,11):\n",
    "            \n",
    "            t = time.process_time()\n",
    "\n",
    "            self.model.fit(X_train, {\"prediction\":Y_train},validation_split=0.2,verbose=0,epochs=100)\n",
    "\n",
    "            fit_times.append(time.process_time() - t)\n",
    "        \n",
    "            t = time.process_time()\n",
    "\n",
    "            Y_pred = self.model.predict(X_test)\n",
    "            \n",
    "            #sns.residplot(Y_pred.flatten(), getResiduals(Y_test,Y_pred))\n",
    "\n",
    "            pred_times.append(time.process_time() - t)\n",
    "            \n",
    "            errs.append(self.computeError (Y_pred, Y_test))\n",
    "        \n",
    "        self.fit_time = sum(fit_times)/len(fit_times)\n",
    "        \n",
    "        self.pred_time = sum(pred_times)/len(pred_times)\n",
    "        \n",
    "        #computeAndPrintError(Y_pred, Y_test)\n",
    "\n",
    "        self.df_err = pd.DataFrame(errs, columns = ['TAE', 'TSE', 'MAE', 'MSE', 'MAPE', '#Pts'])\n",
    "        \n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "expExectr = DNNExperimentExecutor()\n",
    "expExectr.executeExperiments(dm, \"Simple_DNN\", df_dnnexperimentTracker)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_perm_feature_importance(expExectr.model, expExectr.dm.X_test, expExectr.dm.Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "df_dnnexperimentTracker.to_csv('PAPER1_PCA_GP_DNN_NEIGHBORDNN_PCDNN_V2_Experiments_SimpleDNN.csv', sep='\\t',encoding='utf-8', index=False)\n",
    "\n",
    "#!!!!!!! Very Important !!!!!!\n",
    "resultsFile.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCDNN Ablation Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCDNNExperimentExecutor:\n",
    "    def __init__(self):\n",
    "        self.dm = None\n",
    "        self.modelType = None\n",
    "        self.model = None\n",
    "        self.df_experimentTracker = None\n",
    "        self.fit_time = None\n",
    "        self.pred_time = None\n",
    "        self.err = None\n",
    "        self.df_err = None \n",
    "        self.width = 512\n",
    "        self.halfwidth = 128 \n",
    "        self.predicitions = None\n",
    "    \n",
    "    def setModel(self,model):\n",
    "        self.model = model\n",
    "        \n",
    "    def getPredicitons(self):\n",
    "        return self.predicitions\n",
    "\n",
    "    def build_and_compile_pcdnn_v1_wo_zmix_model(self,noOfInputNeurons,noOfCpv):\n",
    "\n",
    "        species_inputs = keras.Input(shape=(noOfInputNeurons,), name=\"species_input\")\n",
    "\n",
    "        linear_reduced_dims = layers.Dense(noOfCpv, name=\"linear_layer\")(species_inputs)\n",
    "\n",
    "        x = layers.Dense(32, activation=\"relu\")(linear_reduced_dims)\n",
    "        x = layers.Dense(64, activation=\"relu\")(x)\n",
    "        x = layers.Dense(128, activation=\"relu\")(x)\n",
    "        x = layers.Dense(256, activation=\"relu\")(x)\n",
    "        x = layers.Dense(512, activation=\"relu\")(x)\n",
    "        x = layers.Dense(256, activation=\"relu\")(x)\n",
    "        x = layers.Dense(128, activation=\"relu\")(x)\n",
    "        x = layers.Dense(64, activation=\"relu\")(x)\n",
    "        x = layers.Dense(32, activation=\"relu\")(x)\n",
    "        #Predict the source energy\n",
    "        souener_pred = layers.Dense(1, name=\"prediction\")(x)\n",
    "\n",
    "        physics_pred = layers.Dense(noOfCpv, name=\"physics\")(linear_reduced_dims)\n",
    "        \n",
    "        model = keras.Model(inputs=[species_inputs],outputs=[souener_pred,physics_pred])\n",
    "\n",
    "        opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "        \n",
    "        model.compile(loss={\"physics\": keras.losses.MeanAbsoluteError(),\"prediction\": keras.losses.MeanAbsoluteError()},loss_weights=[2.0, 0.2],optimizer=opt)\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def build_and_compile_pcdnn_v1_with_zmix_model(self,noOfInputNeurons,noOfCpv):\n",
    "\n",
    "        species_inputs = keras.Input(shape=(noOfInputNeurons,), name=\"species_input\")\n",
    "        \n",
    "        linear_reduced_dims = layers.Dense(noOfCpv, name=\"linear_layer\")(species_inputs)\n",
    "\n",
    "        zmix = keras.Input(shape=(1,), name=\"zmix\")\n",
    "            \n",
    "        x = layers.concatenate([linear_reduced_dims,zmix])\n",
    "        x = layers.Dense(32, activation=\"relu\")(x)\n",
    "        x = layers.Dense(64, activation=\"relu\")(x)\n",
    "        x = layers.Dense(128, activation=\"relu\")(x)\n",
    "        x = layers.Dense(256, activation=\"relu\")(x)\n",
    "        x = layers.Dense(512, activation=\"relu\")(x)\n",
    "        x = layers.Dense(256, activation=\"relu\")(x)\n",
    "        x = layers.Dense(128, activation=\"relu\")(x)\n",
    "        x = layers.Dense(64, activation=\"relu\")(x)\n",
    "        x = layers.Dense(32, activation=\"relu\")(x)\n",
    "        #Predict the source energy\n",
    "        souener_pred = layers.Dense(1, name=\"prediction\")(x)\n",
    "\n",
    "        physics_pred = layers.Dense(noOfCpv, name=\"physics\")(linear_reduced_dims)\n",
    "        \n",
    "        model = keras.Model(inputs=[species_inputs,zmix],outputs=[souener_pred,physics_pred])\n",
    "\n",
    "        opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "        \n",
    "        model.compile(loss={\"physics\": keras.losses.MeanAbsoluteError(),\"prediction\": keras.losses.MeanAbsoluteError()},loss_weights=[2.0, 0.2],optimizer=opt)\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def executeExperiment(self,dataManager, modelType, dataType=\"randomequaltraintestsplit\",inputType=\"ZmixPCA\",noOfCpv=5):\n",
    "        self.dm = dataManager\n",
    "        \n",
    "        self.modelType = modelType\n",
    "        \n",
    "        dataSetMethod = inputType + '_' + dataType\n",
    "        \n",
    "        self.dm.createTrainTestData(dataSetMethod,noOfCpv, None, None)\n",
    "        \n",
    "        #(self,X_train, Y_train, X_test, Y_test, rom_train = None, rom_test = None, zmix_train = None, zmix_test = None, Y_scaler = None)\n",
    "        self.fitModelAndCalcErr(self.dm.X_train, self.dm.Y_train, self.dm.X_test, self.dm.Y_test,self.dm.rom_train, self.dm.rom_test, self.dm.zmix_train, self.dm.zmix_test, self.dm.outputScaler)\n",
    "        \n",
    "        if inputType.find('Zmix') != -1:\n",
    "            ZmixPresent = 'Y'\n",
    "        else:\n",
    "            ZmixPresent = 'N'\n",
    "\n",
    "                            #['Model','Dataset','Cpv Type','#Cpv',\"ZmixExists\",'MAE','TAE','MSE','TSE','#Pts','FitTime','PredTime','MAX-MAE','MAX-TAE','MAX-MSE','MAX-TSE','MIN-MAE','MIN-TAE','MIN-MSE','MIN-TSE']\n",
    "\n",
    "        experimentResults = [self.modelType, dataType,inputType,str(noOfCpv), ZmixPresent,str(self.df_err['MAE'].mean()),str(self.df_err['TAE'].mean()),str(self.df_err['MSE'].mean()),str(self.df_err['TSE'].mean()),str(self.df_err['#Pts'].mean()),str(self.fit_time),str(self.pred_time),str(self.df_err['MAE'].max()),str(self.df_err['TAE'].max()),str(self.df_err['MSE'].max()),str(self.df_err['TSE'].max()),str(self.df_err['MAE'].min()),str(self.df_err['TAE'].min()),str(self.df_err['MSE'].min()),str(self.df_err['TSE'].min())]\n",
    "\n",
    "        printStr = \"self.modelType: \"+ self.modelType+ \" dataType: \"  + dataType+ \" inputType:\"+inputType+ \" noOfCpv:\"+str(noOfCpv)+ \" ZmixPresent:\" + ZmixPresent + \" MAE:\" +str(self.err[2])\n",
    "\n",
    "        print(printStr)\n",
    "    \n",
    "    def executeSingleExperiment(self,noOfInputNeurons,dataSetMethod,dataType,inputType,ZmixPresent,noOfCpv,concatenateZmix):\n",
    "        \n",
    "        \n",
    "        #                           dataSetMethod,noOfCpvs, ipscaler, opscaler\n",
    "        self.dm.createTrainTestData(dataSetMethod,noOfCpv, None, \"MinMaxScaler\")\n",
    "        \n",
    "        if concatenateZmix == 'N':\n",
    "            print(\"--------------------self.build_and_compile_pcdnn_v1_wo_zmix_model----------------------\")\n",
    "            self.model = self.build_and_compile_pcdnn_v1_wo_zmix_model(noOfInputNeurons,noOfCpv)\n",
    "            \n",
    "        else:\n",
    "            self.model = self.build_and_compile_pcdnn_v1_with_zmix_model(noOfInputNeurons,noOfCpv)\n",
    "\n",
    "        \n",
    "        self.model.summary()\n",
    "\n",
    "        #(self,X_train, Y_train, X_test, Y_test, rom_train = None, rom_test = None, zmix_train = None, zmix_test = None, Y_scaler = None)\n",
    "        if self.dm.inputScaler is not None:\n",
    "            X_train = self.dm.X_scaled_train\n",
    "            X_test = self.dm.X_scaled_test\n",
    "            zmix_train = self.dm.zmix_scaled_train\n",
    "            zmix_test = self.dm.zmix_scaled_test\n",
    "        else:\n",
    "            X_train = self.dm.X_train\n",
    "            X_test = self.dm.X_test\n",
    "            zmix_train = self.dm.zmix_train\n",
    "            zmix_test = self.dm.zmix_test\n",
    "        \n",
    "        if self.dm.outputScaler is not None:\n",
    "            Y_train = self.dm.Y_scaled_train\n",
    "            Y_test = self.dm.Y_scaled_test\n",
    "            rom_train = self.dm.rom_scaled_train\n",
    "            rom_test = self.dm.rom_scaled_test\n",
    "        else:\n",
    "            Y_train = self.dm.Y_train\n",
    "            Y_test = self.dm.Y_test\n",
    "            rom_train = self.dm.rom_train\n",
    "            rom_test = self.dm.rom_test\n",
    "            \n",
    "        self.fitModelAndCalcErr(X_train, Y_train, X_test, Y_test,rom_train, rom_test, zmix_train, zmix_test, self.dm.outputScaler, concatenateZmix)\n",
    "\n",
    "        #['Model','Dataset','Cpv Type','#Cpv',\"ZmixExists\",'MAE','TAE','MSE','TSE','#Pts','FitTime','PredTime','MAX-MAE','MAX-TAE','MAX-MSE','MAX-TSE','MIN-MAE','MIN-TAE','MIN-MSE','MIN-TSE']\n",
    "\n",
    "        experimentResults = [self.modelType, dataType,inputType,str(noOfCpv), ZmixPresent,str(self.df_err['MAE'].mean()),str(self.df_err['TAE'].mean()),str(self.df_err['MSE'].mean()),str(self.df_err['TSE'].mean()),str(self.df_err['#Pts'].mean()),str(self.fit_time),str(self.pred_time),str(self.df_err['MAE'].max()),str(self.df_err['TAE'].max()),str(self.df_err['MSE'].max()),str(self.df_err['TSE'].max()),str(self.df_err['MAE'].min()),str(self.df_err['TAE'].min()),str(self.df_err['MSE'].min()),str(self.df_err['TSE'].min())]\n",
    "\n",
    "        self.df_experimentTracker.loc[len(self.df_experimentTracker)] = experimentResults        \n",
    "\n",
    "        printStr = \"self.modelType: \"+ self.modelType+ \" dataType: \"  + dataType+ \" inputType:\"+inputType+ \" noOfCpv:\"+str(noOfCpv)+ \" ZmixPresent:\" + ZmixPresent + \" MAE:\" +str(self.df_err['MAE'].min())\n",
    "\n",
    "        print(printStr)\n",
    "\n",
    "        printStr = \"\\t\"\n",
    "\n",
    "        printStr = printStr.join(experimentResults)\n",
    "\n",
    "        resultsFile.write(printStr)\n",
    "\n",
    "        resultsFile.write(\"\\n\")\n",
    "\n",
    "\n",
    "    def fitModelAndCalcErr(self,X_train, Y_train, X_test, Y_test, rom_train = None, rom_test = None, zmix_train = None, zmix_test = None, Y_scaler = None, concatenateZmix = 'N'):\n",
    "\n",
    "        fit_times = []\n",
    "        \n",
    "        pred_times = []\n",
    "        \n",
    "        errs = []\n",
    "        \n",
    "        for itr in range(1,11):\n",
    "            \n",
    "            t = time.process_time()\n",
    "\n",
    "            if concatenateZmix == 'Y':\n",
    "                history = self.model.fit({\"species_input\":X_train, \"zmix\":zmix_train}, {\"physics\":rom_train,\"prediction\":Y_train},validation_split=0.2,verbose=0,epochs=100)\n",
    "            else:\n",
    "                history = self.model.fit({\"species_input\":X_train}, {\"physics\":rom_train,\"prediction\":Y_train},validation_split=0.2,verbose=0,epochs=100)\n",
    "            \n",
    "            #self.plot_loss_physics_and_regression(history)\n",
    "            \n",
    "            fit_times.append(time.process_time() - t)\n",
    "        \n",
    "            t = time.process_time()\n",
    "\n",
    "            if concatenateZmix == 'Y':\n",
    "                predictions = self.model.predict({\"species_input\":X_test, \"zmix\":zmix_test})\n",
    "            else:\n",
    "                predictions = self.model.predict({\"species_input\":X_test})\n",
    "                \n",
    "            pred_times.append(time.process_time() - t)\n",
    "            \n",
    "            self.predicitions = predictions\n",
    "            \n",
    "            Y_pred = predictions[0]\n",
    "            \n",
    "\n",
    "            if Y_scaler is not None:\n",
    "                Y_pred = Y_scaler.inverse_transform(Y_pred)\n",
    "                \n",
    "                \n",
    "            #sns.residplot(Y_pred.flatten(), getResiduals(Y_test,Y_pred))\n",
    "\n",
    "            errs.append(self.computeError (Y_pred, Y_test))\n",
    "        \n",
    "        self.fit_time = sum(fit_times)/len(fit_times)\n",
    "        \n",
    "        self.pred_time = sum(pred_times)/len(pred_times)\n",
    "        \n",
    "        #computeAndPrintError(Y_pred, Y_test)\n",
    "\n",
    "        self.df_err = pd.DataFrame(errs, columns = ['TAE', 'TSE', 'MAE', 'MSE', 'MAPE', '#Pts'])\n",
    "        \n",
    "        return  \n",
    "\n",
    "    def executeExperiments(self,dataManager, modelType, df_experimentTracker):\n",
    "        self.dm = dataManager\n",
    "        self.modelType = modelType\n",
    "        self.df_experimentTracker = df_experimentTracker\n",
    "        \n",
    "        #Experiments  \n",
    "        \n",
    "        #dataTypes = [\"randomequaltraintestsplit\",\"frameworkincludedtrainexcludedtest\"]\n",
    "        dataTypes = [\"frameworkincludedtrainexcludedtest\"]\n",
    "        #dataTypes = [\"randomequaltraintestsplit\"]\n",
    "        \n",
    "        inputTypes = [\"AllSpeciesZmixCpv\",\"AllSpeciesZmixPCA\",\"AllSpeciesPurePCA\",\"AllSpeciesSparsePCA\",\"AllSpeciesZmixAndPurePCA\",\"AllSpeciesZmixAndSparsePCA\"]\n",
    "        \n",
    "        #inputTypes = [\"AllSpeciesZmixAndPurePCA\"]\n",
    "        \n",
    "        concatenateZmix = 'N'\n",
    "        \n",
    "        for dataType in dataTypes:\n",
    "            print('=================== ' + dataType + ' ===================')\n",
    "            \n",
    "            for inputType in inputTypes:\n",
    "                \n",
    "                print('------------------ ' + inputType + ' ------------------')\n",
    "                    \n",
    "                #ZmixCpv_randomequaltraintestsplit\n",
    "                dataSetMethod = inputType + '_' + dataType\n",
    "                \n",
    "                noOfNeurons = 53\n",
    "\n",
    "                #ZmixAnd & ZmixAll\n",
    "                if inputType.find('ZmixA') != -1:\n",
    "                    concatenateZmix = 'Y'\n",
    "                else:\n",
    "                    concatenateZmix = 'N'\n",
    "\n",
    "                if inputType.find('Zmix') != -1:\n",
    "                    ZmixPresent = 'Y'\n",
    "                else:\n",
    "                    ZmixPresent = 'N'\n",
    "                    \n",
    "                \n",
    "                if inputType.find('PCA') != -1:\n",
    "                    \n",
    "                    noOfCpvs = [item for item in range(1, 6)]\n",
    "                    \n",
    "                    for noOfCpv in noOfCpvs:\n",
    "                        \n",
    "                        self.executeSingleExperiment(noOfNeurons,dataSetMethod,dataType,inputType,ZmixPresent,noOfCpv,concatenateZmix)\n",
    "                else:\n",
    "\n",
    "                    if inputType.find('ZmixCpv') != -1:\n",
    "                        noOfCpv = 1\n",
    "                 \n",
    "                    else:\n",
    "                        noOfCpv = 53\n",
    "                 \n",
    "                    print('------------------ ' + str(noOfNeurons) + ' ------------------')\n",
    "                    self.executeSingleExperiment(noOfNeurons,dataSetMethod,dataType,inputType,ZmixPresent,noOfCpv,concatenateZmix)\n",
    "        \n",
    "    def computeError (self,Y_pred, Y_test):\n",
    "        evaluation_df_1 = pd.DataFrame()\n",
    "\n",
    "        evaluation_df_1['souener'] = Y_test.flatten()\n",
    "\n",
    "        evaluation_df_1['souener_pred'] = Y_pred.flatten()\n",
    "\n",
    "        evaluation_df_1['souener_pred_L1'] = evaluation_df_1['souener'] - evaluation_df_1['souener_pred'] \n",
    "\n",
    "        evaluation_df_1['souener_pred_L2'] = evaluation_df_1['souener_pred_L1'] * evaluation_df_1['souener_pred_L1']\n",
    "\n",
    "        evaluation_df_1['souener_pred_L1Percent'] = ((evaluation_df_1['souener'] - evaluation_df_1['souener_pred'])/evaluation_df_1['souener']) \n",
    "\n",
    "        TotalAbsoluteError = evaluation_df_1['souener_pred_L1'].abs().sum()\n",
    "\n",
    "        TotalSquaredError = evaluation_df_1['souener_pred_L2'].abs().sum()\n",
    "\n",
    "        MeanAbsoluteError = evaluation_df_1['souener_pred_L1'].abs().sum()/evaluation_df_1['souener_pred_L1'].abs().count()\n",
    "\n",
    "        MeanSquaredError = evaluation_df_1['souener_pred_L2'].abs().sum()/evaluation_df_1['souener_pred_L2'].abs().count()\n",
    "\n",
    "        NumPoints = evaluation_df_1['souener_pred_L1Percent'].abs().count()\n",
    "\n",
    "        MeanPercentageError = evaluation_df_1['souener_pred_L1Percent'].abs().sum()/NumPoints\n",
    "\n",
    "        return [TotalAbsoluteError,TotalSquaredError,MeanAbsoluteError,MeanSquaredError,MeanPercentageError,NumPoints]\n",
    "\n",
    "            \n",
    "    def printError (self,err):\n",
    "        TotalAbsoluteError = err[0]\n",
    "\n",
    "        TotalSquaredError = err[1]\n",
    "\n",
    "        MeanAbsoluteError = err[2]\n",
    "\n",
    "        MeanSquaredError = err[3]\n",
    "\n",
    "        MeanPercentageError = err[4]\n",
    "\n",
    "        NumPoints = err[5]\n",
    "        \n",
    "        print ('Total Absolute Error: ', TotalAbsoluteError)\n",
    "        print ('Mean Absolute Error: ', MeanAbsoluteError)\n",
    "        print ('Mean Percentage Error: ', MeanPercentageError)\n",
    "        print ('Total Squared Error: ', TotalSquaredError)\n",
    "        print ('Mean Squared Error: ', MeanSquaredError)\n",
    "        print ('Number of Points: ', NumPoints)\n",
    "\n",
    "    def computeAndPrintError(self,Y_pred, Y_test):    \n",
    "        self.err = computeError (Y_pred, Y_test)\n",
    "        printError (err)\n",
    "        logError(err)\n",
    "        return err    \n",
    "        \n",
    "    def plot_loss_physics_and_regression(self,history):\n",
    "        \n",
    "        f = plt.figure(figsize=(10,3))\n",
    "        ax = f.add_subplot(121)\n",
    "        ax2 = f.add_subplot(122)\n",
    "\n",
    "        ax.plot(history.history['prediction_loss'], label='loss')\n",
    "        ax.plot(history.history['val_prediction_loss'], label='val_loss')\n",
    "        ax.set_title('Souener Prediction Loss')\n",
    "        ax.set(xlabel='Epoch', ylabel='Souener Error')\n",
    "        ax.legend()\n",
    "\n",
    "        ax2.plot(history.history['physics_loss'], label='loss')\n",
    "        ax2.plot(history.history['val_physics_loss'], label='val_loss')\n",
    "        ax2.set_title('Physics Loss')\n",
    "        ax2.set(xlabel='Epoch', ylabel='Physics Error')\n",
    "        ax2.legend()    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "dnnexperimentTrackingFields = ['Model','Dataset','Cpv Type','#Cpv',\"ZmixExists\",'MAE','TAE','MSE','TSE','#Pts','FitTime','PredTime','MAX-MAE','MAX-TAE','MAX-MSE','MAX-TSE','MIN-MAE','MIN-TAE','MIN-MSE','MIN-TSE']\n",
    "\n",
    "df_pcdnnexperimentTracker = pd.DataFrame(columns=dnnexperimentTrackingFields)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import os#must import this library\n",
    "\n",
    "if os.path.exists('results.csv'):\n",
    "        os.remove('results.csv') #this deletes the file\n",
    "else:\n",
    "        print(\"The results.csv file does not exist\")\n",
    "\n",
    "resultsFile = open('results.csv', 'w')\n",
    "\n",
    "printStr = \"\\t\"\n",
    "\n",
    "printStr = printStr.join(dnnexperimentTrackingFields)\n",
    "\n",
    "\n",
    "resultsFile.write(printStr)\n",
    "\n",
    "resultsFile.write(\"\\n\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "expExectr = PCDNNExperimentExecutor()\n",
    "\n",
    "expExectr.executeExperiments(dm, \"PCDNN\", df_pcdnnexperimentTracker)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df_pcdnnexperimentTracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "df_pcdnnexperimentTracker.to_csv('PAPER1_PCA_GP_DNN_NEIGHBORDNN_PCDNN_V2_Experiments_PCDNN.csv', sep='\\t',encoding='utf-8', index=False)\n",
    "\n",
    "#!!!!!!! Very Important !!!!!!\n",
    "resultsFile.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCDNN V2 Ablation Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers, activations, initializers, constraints, Sequential\n",
    "from tensorflow.keras.constraints import UnitNorm, Constraint\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "class WeightsOrthogonalityConstraint (Constraint):\n",
    "    def __init__(self, encoding_dim, weightage = 1.0, axis = 0):\n",
    "        self.encoding_dim = encoding_dim\n",
    "        self.weightage = weightage\n",
    "        self.axis = axis\n",
    "        \n",
    "    def weights_orthogonality(self, w):\n",
    "        if(self.axis==1):\n",
    "            w = tf.transpose(w)\n",
    "        if(self.encoding_dim > 1):\n",
    "            m = tf.matmul(tf.transpose(w), w) - tf.eye(self.encoding_dim)\n",
    "            return self.weightage * tf.math.sqrt(tf.math.reduce_sum(tf.math.square(m)))\n",
    "        else:\n",
    "            m = tf.math.reduce_sum(w ** 2) - 1.\n",
    "            return m\n",
    "\n",
    "    def __call__(self, w):\n",
    "        return self.weights_orthogonality(w)\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {'encoding_dim':encoding_dim}\n",
    "    \n",
    "\n",
    "class UncorrelatedFeaturesConstraint (Constraint):\n",
    "\n",
    "    def __init__(self, encoding_dim, weightage=1.0):\n",
    "        self.encoding_dim = encoding_dim\n",
    "        self.weightage = weightage\n",
    "\n",
    "    def get_covariance(self, x):\n",
    "        x_centered_list = []\n",
    "\n",
    "        for i in range(self.encoding_dim):\n",
    "            x_centered_list.append(x[:, i] - tf.math.reduce_mean(x[:, i]))\n",
    "\n",
    "        x_centered = tf.stack(x_centered_list)\n",
    "        covariance = tf.matmul(x_centered, tf.transpose(x_centered)) / \\\n",
    "            tf.cast(x_centered.get_shape()[0], tf.float32)\n",
    "\n",
    "        return covariance\n",
    "\n",
    "    # Constraint penalty\n",
    "    def uncorrelated_feature(self, x):\n",
    "        if(self.encoding_dim <= 1):\n",
    "            return 0.0\n",
    "        else:\n",
    "            output = tf.math.reduce_sum(tf.math.square(\n",
    "                self.covariance - tf.math.multiply(self.covariance, tf.eye(self.encoding_dim))))\n",
    "            return output\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.covariance = self.get_covariance(x)\n",
    "        return self.weightage * self.uncorrelated_feature(x)\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {'weightage': self.weightage, 'encoding_dim':encoding_dim}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "class PCDNNV2ExperimentExecutor:\n",
    "    def __init__(self):\n",
    "        self.dm = None\n",
    "        self.modelType = None\n",
    "        self.model = None\n",
    "        self.df_experimentTracker = None\n",
    "        self.fit_time = None\n",
    "        self.pred_time = None\n",
    "        self.err = None\n",
    "        self.df_err = None \n",
    "        self.width = 512\n",
    "        self.halfwidth = 128 \n",
    "        self.predicitions = None\n",
    "        self.fit_iterations = 10\n",
    "    \n",
    "    def setModel(self,model):\n",
    "        self.model = model\n",
    "        \n",
    "    def getPredicitons(self):\n",
    "        return self.predicitions\n",
    "\n",
    "    def build_and_compile_pcdnn_v1_wo_zmix_model(self,noOfInputNeurons,noOfCpv,kernel_constraint='Y',kernel_regularizer='Y',activity_regularizer='Y'):\n",
    "\n",
    "        print (noOfInputNeurons,noOfCpv,kernel_constraint,kernel_regularizer,activity_regularizer)\n",
    "        \n",
    "        species_inputs = keras.Input(shape=(noOfInputNeurons,), name=\"species_input\")\n",
    "        \n",
    "        x = self.getLinearLayer(noOfInputNeurons,noOfCpv,kernel_constraint,kernel_regularizer,activity_regularizer)(species_inputs)\n",
    "        \n",
    "        x = layers.Dense(32, activation=\"relu\")(x)\n",
    "        x = layers.Dense(64, activation=\"relu\")(x)\n",
    "        x = layers.Dense(128, activation=\"relu\")(x)\n",
    "        x = layers.Dense(256, activation=\"relu\")(x)\n",
    "        x = layers.Dense(512, activation=\"relu\")(x)\n",
    "        x = layers.Dense(256, activation=\"relu\")(x)\n",
    "        x = layers.Dense(128, activation=\"relu\")(x)\n",
    "        x = layers.Dense(64, activation=\"relu\")(x)\n",
    "        x = layers.Dense(32, activation=\"relu\")(x)\n",
    "        #Predict the source energy\n",
    "        souener_pred = layers.Dense(1, name=\"prediction\")(x)\n",
    "\n",
    "        model = keras.Model(inputs=[species_inputs],outputs=[souener_pred],)\n",
    "\n",
    "        opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "        \n",
    "        model.compile(loss='mean_absolute_error',optimizer=opt)\n",
    "        \n",
    "        return model\n",
    "\n",
    "    \n",
    "    def getLinearLayer(self,noOfInputNeurons,noOfCpv,kernel_constraint='Y',kernel_regularizer='Y',activity_regularizer='Y'):\n",
    "        \n",
    "        if kernel_constraint=='Y'and kernel_regularizer =='N' and activity_regularizer =='N':\n",
    "            layer = layers.Dense(noOfCpv, activation=\"linear\",kernel_constraint=UnitNorm(axis=0))\n",
    "        \n",
    "        elif kernel_constraint=='N'and kernel_regularizer =='Y' and activity_regularizer =='N':\n",
    "            layer = layers.Dense(noOfCpv, activation=\"linear\",kernel_regularizer=WeightsOrthogonalityConstraint(noOfCpv, weightage=1., axis=0))\n",
    "            \n",
    "        elif kernel_constraint=='N'and kernel_regularizer =='N' and activity_regularizer =='Y':\n",
    "            layer = layers.Dense(noOfCpv, activation=\"linear\",activity_regularizer=UncorrelatedFeaturesConstraint(noOfCpv, weightage=1.))\n",
    "        \n",
    "        elif kernel_constraint=='Y'and kernel_regularizer =='Y' and activity_regularizer =='N':\n",
    "            layer = layers.Dense(noOfCpv, activation=\"linear\",kernel_constraint=UnitNorm(axis=0),kernel_regularizer=WeightsOrthogonalityConstraint(noOfCpv, weightage=1., axis=0))\n",
    "        \n",
    "        elif kernel_constraint=='Y'and kernel_regularizer =='N' and activity_regularizer =='Y':\n",
    "            layer = layers.Dense(noOfCpv, activation=\"linear\",kernel_constraint=UnitNorm(axis=0),activity_regularizer=UncorrelatedFeaturesConstraint(noOfCpv, weightage=1.))\n",
    "        \n",
    "        elif kernel_constraint=='N'and kernel_regularizer =='Y' and activity_regularizer =='Y':\n",
    "            layer = layers.Dense(noOfCpv, activation=\"linear\",kernel_regularizer=WeightsOrthogonalityConstraint(noOfCpv, weightage=1., axis=0),activity_regularizer=UncorrelatedFeaturesConstraint(noOfCpv, weightage=1.))\n",
    "        \n",
    "        elif kernel_constraint=='N'and kernel_regularizer =='N' and activity_regularizer =='N':\n",
    "            layer = layers.Dense(noOfCpv, activation=\"linear\")\n",
    "                \n",
    "        else:\n",
    "            layer = layers.Dense(noOfCpv, activation=\"linear\",kernel_constraint=UnitNorm(axis=0),kernel_regularizer=WeightsOrthogonalityConstraint(noOfCpv, weightage=1., axis=0),activity_regularizer=UncorrelatedFeaturesConstraint(noOfCpv, weightage=1.))\n",
    "        return layer\n",
    "\n",
    "    def build_and_compile_pcdnn_v1_with_zmix_model(self,noOfInputNeurons,noOfCpv,kernel_constraint='Y',kernel_regularizer='Y',activity_regularizer='Y'):\n",
    "        K.clear_backend()\n",
    "        species_inputs = keras.Input(shape=(noOfInputNeurons,), name=\"species_input\")\n",
    "\n",
    "        Zmix = keras.Input(shape=(1,), name=\"Zmix\")\n",
    "\n",
    "        x = self.getLinearLayer(noOfInputNeurons,noOfCpv,kernel_constraint,kernel_regularizer,activity_regularizer)(species_inputs)\n",
    "\n",
    "        #Concatenate the Linear Embedding and Zmix together\n",
    "        x = layers.Concatenate()([Zmix, x])\n",
    "\n",
    "        x = layers.Dense(32, activation=\"relu\")(x)\n",
    "        x = layers.Dense(64, activation=\"relu\")(x)\n",
    "        x = layers.Dense(128, activation=\"relu\")(x)\n",
    "        x = layers.Dense(256, activation=\"relu\")(x)\n",
    "        x = layers.Dense(512, activation=\"relu\")(x)\n",
    "        x = layers.Dense(256, activation=\"relu\")(x)\n",
    "        x = layers.Dense(128, activation=\"relu\")(x)\n",
    "        x = layers.Dense(64, activation=\"relu\")(x)\n",
    "        x = layers.Dense(32, activation=\"relu\")(x)\n",
    "        #Predict the source energy\n",
    "        souener_pred = layers.Dense(1, name=\"prediction\")(x)\n",
    "\n",
    "        model = keras.Model(inputs=[species_inputs,Zmix],outputs=[souener_pred],)\n",
    "\n",
    "        opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "        \n",
    "        model.compile(loss='mean_absolute_error',optimizer=opt)\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    \n",
    "    def executeSingleExperiment(self,noOfInputNeurons,dataSetMethod,dataType,inputType,ZmixPresent,noOfCpv,concatenateZmix,kernel_constraint,kernel_regularizer,activity_regularizer):\n",
    "        import pdb\n",
    "        pdb.set_trace()\n",
    "        \n",
    "        #                           dataSetMethod,noOfCpvs, ipscaler, opscaler\n",
    "        self.dm.createTrainTestData(dataSetMethod,noOfCpv, \"MinMaxScaler\", \"MinMaxScaler\")\n",
    "        \n",
    "        if concatenateZmix == 'N':\n",
    "            print(\"--------------------self.build_and_compile_pcdnn_v1_wo_zmix_model----------------------\")\n",
    "            self.model = self.build_and_compile_pcdnn_v1_wo_zmix_model(noOfInputNeurons,noOfCpv,kernel_constraint,kernel_regularizer,activity_regularizer)\n",
    "            \n",
    "        else:\n",
    "            self.model = self.build_and_compile_pcdnn_v1_with_zmix_model(noOfInputNeurons,noOfCpv,kernel_constraint,kernel_regularizer,activity_regularizer)\n",
    "\n",
    "        \n",
    "        self.model.summary()\n",
    "\n",
    "        #(self,X_train, Y_train, X_test, Y_test, rom_train = None, rom_test = None, zmix_train = None, zmix_test = None, Y_scaler = None)\n",
    "        if self.dm.inputScaler is not None:\n",
    "            X_train = self.dm.X_scaled_train\n",
    "            X_test = self.dm.X_scaled_test\n",
    "            zmix_train = self.dm.zmix_scaled_train\n",
    "            zmix_test = self.dm.zmix_scaled_test\n",
    "        else:\n",
    "            X_train = self.dm.X_train\n",
    "            X_test = self.dm.X_test\n",
    "            zmix_train = self.dm.zmix_train\n",
    "            zmix_test = self.dm.zmix_test\n",
    "        \n",
    "        if self.dm.outputScaler is not None:\n",
    "            Y_train = self.dm.Y_scaled_train\n",
    "            Y_test = self.dm.Y_scaled_test\n",
    "        else:\n",
    "            Y_train = self.dm.Y_train\n",
    "            Y_test = self.dm.Y_test\n",
    "\n",
    "            \n",
    "        self.fitModelAndCalcErr(X_train, Y_train, X_test, Y_test,None, None, zmix_train, zmix_test, self.dm.outputScaler, concatenateZmix,kernel_constraint,kernel_regularizer,activity_regularizer)\n",
    "\n",
    "        #['Model','Dataset','Cpv Type','#Cpv',\"ZmixExists\",'MAE','TAE','MSE','TSE','#Pts','FitTime','PredTime','MAX-MAE','MAX-TAE','MAX-MSE','MAX-TSE','MIN-MAE','MIN-TAE','MIN-MSE','MIN-TSE']\n",
    "\n",
    "        experimentResults = [self.modelType, dataType,inputType,str(noOfCpv), ZmixPresent,kernel_constraint,kernel_regularizer,activity_regularizer,str(self.df_err['MAE'].mean()),str(self.df_err['TAE'].mean()),str(self.df_err['MSE'].mean()),str(self.df_err['TSE'].mean()),str(self.df_err['#Pts'].mean()),str(self.fit_time),str(self.pred_time),str(self.df_err['MAE'].max()),str(self.df_err['TAE'].max()),str(self.df_err['MSE'].max()),str(self.df_err['TSE'].max()),str(self.df_err['MAE'].min()),str(self.df_err['TAE'].min()),str(self.df_err['MSE'].min()),str(self.df_err['TSE'].min())]\n",
    "\n",
    "        self.df_experimentTracker.loc[len(self.df_experimentTracker)] = experimentResults        \n",
    "\n",
    "        printStr = \"self.modelType: \"+ self.modelType+ \" dataType: \"  + dataType+ \" inputType:\"+inputType+ \" noOfCpv:\"+str(noOfCpv)+ \" ZmixPresent:\" + ZmixPresent + \" MAE:\" +str(self.df_err['MAE'].min())\n",
    "\n",
    "        print(printStr)\n",
    "\n",
    "        printStr = \"\\t\"\n",
    "\n",
    "        printStr = printStr.join(experimentResults)\n",
    "\n",
    "        resultsFile.write(printStr)\n",
    "\n",
    "        resultsFile.write(\"\\n\")\n",
    "\n",
    "\n",
    "    def fitModelAndCalcErr(self, X_train, Y_train, X_test, Y_test,\n",
    "                           rom_train = None, rom_test = None, zmix_train = None, zmix_test = None, Y_scaler = None,\n",
    "                           concatenateZmix = 'N',kernel_constraint = 'Y',kernel_regularizer = 'Y',activity_regularizer = 'Y'):\n",
    "\n",
    "        fit_times = []\n",
    "        \n",
    "        pred_times = []\n",
    "        \n",
    "        errs = []\n",
    "        \n",
    "        for itr in range(1,self.fit_iterations+1):\n",
    "            \n",
    "            t = time.process_time()\n",
    "\n",
    "            if concatenateZmix == 'Y':\n",
    "                history = self.model.fit({\"species_input\":X_train, \"zmix\":zmix_train}, {\"prediction\":Y_train},validation_split=0.2,verbose=1,epochs=100)\n",
    "            else:\n",
    "                history = self.model.fit({\"species_input\":X_train}, {\"prediction\":Y_train},validation_split=0.2,verbose=1,epochs=100)\n",
    "            \n",
    "            #self.plot_loss_physics_and_regression(history)\n",
    "            \n",
    "            fit_times.append(time.process_time() - t)\n",
    "        \n",
    "            t = time.process_time()\n",
    "\n",
    "            if concatenateZmix == 'Y':\n",
    "                predictions = self.model.predict({\"species_input\":X_test, \"zmix\":zmix_test})\n",
    "            else:\n",
    "                predictions = self.model.predict({\"species_input\":X_test})\n",
    "                \n",
    "            pred_times.append(time.process_time() - t)\n",
    "            \n",
    "            self.predicitions = predictions\n",
    "            \n",
    "            Y_pred = predictions\n",
    "            \n",
    "\n",
    "            if Y_scaler is not None:\n",
    "                Y_pred = Y_scaler.inverse_transform(Y_pred)\n",
    "                \n",
    "                \n",
    "            #sns.residplot(Y_pred.flatten(), getResiduals(Y_test,Y_pred))\n",
    "\n",
    "            errs.append(self.computeError (Y_pred, Y_test))\n",
    "        \n",
    "        self.fit_time = sum(fit_times)/len(fit_times)\n",
    "        self.pred_time = sum(pred_times)/len(pred_times)\n",
    "        \n",
    "        #computeAndPrintError(Y_pred, Y_test)\n",
    "\n",
    "        self.df_err = pd.DataFrame(errs, columns = ['TAE', 'TSE', 'MAE', 'MSE', 'MAPE', '#Pts'])\n",
    "        \n",
    "        return  \n",
    "\n",
    "    def executeExperiments(self, dataManager, modelType, df_experimentTracker):\n",
    "        #pdb.set_trace()\n",
    "        self.dm = dataManager\n",
    "        self.modelType = modelType\n",
    "        self.df_experimentTracker = df_experimentTracker\n",
    "        \n",
    "        #Experiments  \n",
    "        \n",
    "        #dataTypes = [\"randomequaltraintestsplit\",\"frameworkincludedtrainexcludedtest\"]\n",
    "        #dataTypes = [\"frameworkincludedtrainexcludedtest\"]\n",
    "        dataTypes = [\"randomequaltraintestsplit\"]\n",
    "        \n",
    "        #inputTypes = [\"AllSpeciesAndZmix\"]\n",
    "        \n",
    "        inputTypes = [\"AllSpecies\",\"AllSpeciesAndZmix\"]\n",
    "        \n",
    "        concatenateZmix = 'N'\n",
    "        kernel_constraints = ['Y','N']\n",
    "        kernel_regularizers = ['Y','N']\n",
    "        activity_regularizers = ['Y','N']\n",
    "        models = []\n",
    "        for dataType in dataTypes:\n",
    "            print('=================== ' + dataType + ' ===================')\n",
    "            \n",
    "            for inputType in inputTypes:\n",
    "                \n",
    "                print('------------------ ' + inputType + ' ------------------')\n",
    "                    \n",
    "                #ZmixCpv_randomequaltraintestsplit\n",
    "                dataSetMethod = inputType + '_' + dataType\n",
    "                \n",
    "                noOfNeurons = 53\n",
    "\n",
    "                if inputType.find('Zmix') != -1:\n",
    "                    ZmixPresent = 'Y'\n",
    "                    concatenateZmix = 'Y'\n",
    "                else:\n",
    "                    ZmixPresent = 'N'\n",
    "                    concatenateZmix = 'N'\n",
    "                    \n",
    "                noOfCpvs = [item for item in range(2, 6)]\n",
    "\n",
    "                for noOfCpv in noOfCpvs:\n",
    "                    for kernel_constraint in kernel_constraints:\n",
    "                        for kernel_regularizer in kernel_regularizers:\n",
    "                            for activity_regularizer in activity_regularizers:\n",
    "                                self.executeSingleExperiment(noOfNeurons,dataSetMethod,dataType,inputType,ZmixPresent,noOfCpv,concatenateZmix,kernel_constraint,kernel_regularizer,activity_regularizer)\n",
    "                                models.append(self.model)\n",
    "        return models\n",
    "        \n",
    "    def computeError (self,Y_pred, Y_test):\n",
    "        evaluation_df_1 = pd.DataFrame()\n",
    "\n",
    "        evaluation_df_1['souener'] = Y_test.flatten()\n",
    "\n",
    "        evaluation_df_1['souener_pred'] = Y_pred.flatten()\n",
    "\n",
    "        evaluation_df_1['souener_pred_L1'] = evaluation_df_1['souener'] - evaluation_df_1['souener_pred'] \n",
    "\n",
    "        evaluation_df_1['souener_pred_L2'] = evaluation_df_1['souener_pred_L1'] * evaluation_df_1['souener_pred_L1']\n",
    "\n",
    "        evaluation_df_1['souener_pred_L1Percent'] = ((evaluation_df_1['souener'] - evaluation_df_1['souener_pred'])/evaluation_df_1['souener']) \n",
    "\n",
    "        TotalAbsoluteError = evaluation_df_1['souener_pred_L1'].abs().sum()\n",
    "\n",
    "        TotalSquaredError = evaluation_df_1['souener_pred_L2'].abs().sum()\n",
    "\n",
    "        MeanAbsoluteError = evaluation_df_1['souener_pred_L1'].abs().sum()/evaluation_df_1['souener_pred_L1'].abs().count()\n",
    "\n",
    "        MeanSquaredError = evaluation_df_1['souener_pred_L2'].abs().sum()/evaluation_df_1['souener_pred_L2'].abs().count()\n",
    "\n",
    "        NumPoints = evaluation_df_1['souener_pred_L1Percent'].abs().count()\n",
    "\n",
    "        MeanPercentageError = evaluation_df_1['souener_pred_L1Percent'].abs().sum()/NumPoints\n",
    "\n",
    "        return [TotalAbsoluteError,TotalSquaredError,MeanAbsoluteError,MeanSquaredError,MeanPercentageError,NumPoints]\n",
    "\n",
    "            \n",
    "    def printError (self,err):\n",
    "        TotalAbsoluteError = err[0]\n",
    "\n",
    "        TotalSquaredError = err[1]\n",
    "\n",
    "        MeanAbsoluteError = err[2]\n",
    "\n",
    "        MeanSquaredError = err[3]\n",
    "\n",
    "        MeanPercentageError = err[4]\n",
    "\n",
    "        NumPoints = err[5]\n",
    "        \n",
    "        print ('Total Absolute Error: ', TotalAbsoluteError)\n",
    "        print ('Mean Absolute Error: ', MeanAbsoluteError)\n",
    "        print ('Mean Percentage Error: ', MeanPercentageError)\n",
    "        print ('Total Squared Error: ', TotalSquaredError)\n",
    "        print ('Mean Squared Error: ', MeanSquaredError)\n",
    "        print ('Number of Points: ', NumPoints)\n",
    "\n",
    "    def computeAndPrintError(self, Y_pred, Y_test):    \n",
    "        self.err = computeError (Y_pred, Y_test)\n",
    "        printError (err)\n",
    "        logError(err)\n",
    "        return err    \n",
    "        \n",
    "    def plot_loss_physics_and_regression(self,history):\n",
    "        \n",
    "        f = plt.figure(figsize=(10,3))\n",
    "        ax = f.add_subplot(121)\n",
    "        ax2 = f.add_subplot(122)\n",
    "\n",
    "        ax.plot(history.history['prediction_loss'], label='loss')\n",
    "        ax.plot(history.history['val_prediction_loss'], label='val_loss')\n",
    "        ax.set_title('Souener Prediction Loss')\n",
    "        ax.set(xlabel='Epoch', ylabel='Souener Error')\n",
    "        ax.legend()\n",
    "\n",
    "        ax2.plot(history.history['physics_loss'], label='loss')\n",
    "        ax2.plot(history.history['val_physics_loss'], label='val_loss')\n",
    "        ax2.set_title('Physics Loss')\n",
    "        ax2.set(xlabel='Epoch', ylabel='Physics Error')\n",
    "        ax2.legend()    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dnnexperimentTrackingFields = ['Model', 'Dataset', 'Cpv Type', '#Cpv', \"ZmixExists\", 'KernelConstraintExists', 'KernelRegularizerExists', 'ActivityRegularizerExists', 'MAE', 'TAE', 'MSE', 'TSE', '#Pts', 'FitTime', 'PredTime', 'MAX-MAE', 'MAX-TAE', 'MAX-MSE', 'MAX-TSE', 'MIN-MAE', 'MIN-TAE', 'MIN-MSE', 'MIN-TSE']\n",
    "\n",
    "df_pcdnnexperimentTracker = pd.DataFrame(columns=dnnexperimentTrackingFields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import os#must import this library\n",
    "\n",
    "if os.path.exists('results.csv'):\n",
    "        os.remove('results.csv') #this deletes the file\n",
    "else:\n",
    "        print(\"The results.csv file does not exist\")\n",
    "\n",
    "resultsFile = open('results.csv', 'w')\n",
    "\n",
    "printStr = \"\\t\"\n",
    "\n",
    "printStr = printStr.join(dnnexperimentTrackingFields)\n",
    "resultsFile.write(printStr)\n",
    "resultsFile.write(\"\\n\")\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "expExectr = PCDNNV2ExperimentExecutor()\n",
    "expExectr.fit_iterations=1\n",
    "expExectr.executeExperiments(dm, \"PCDNN_V2\", df_pcdnnexperimentTracker)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "do_perm_feature_importance(expExectr.model, expExectr.dm.X_test, expExectr.dm.Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pcdnnexperimentTracker.to_csv('PAPER1_PCA_GP_DNN_NEIGHBORDNN_PCDNN_V2_Experiments_PCDNN.csv', sep='\\t',encoding='utf-8', index=False)\n",
    "resultsFile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dwyerdei/rom_project\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "df_pcdnnexperimentTracker = pd.read_csv('./PAPER1_PCA_GP_DNN_NEIGHBORDNN_PCDNN_V2_Experiments_PCDNN.csv', sep='\\t',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Species DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Model',\n",
       " 'Dataset',\n",
       " 'Cpv Type',\n",
       " '#Cpv',\n",
       " 'ZmixExists',\n",
       " 'KernelConstraintExists',\n",
       " 'KernelRegularizerExists',\n",
       " 'ActivityRegularizerExists',\n",
       " 'MAE',\n",
       " 'TAE',\n",
       " 'MSE',\n",
       " 'TSE',\n",
       " '#Pts',\n",
       " 'FitTime',\n",
       " 'PredTime',\n",
       " 'MAX-MAE',\n",
       " 'MAX-TAE',\n",
       " 'MAX-MSE',\n",
       " 'MAX-TSE',\n",
       " 'MIN-MAE',\n",
       " 'MIN-TAE',\n",
       " 'MIN-MSE',\n",
       " 'MIN-TSE']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_allSpecies = df[icovariates].values\n",
    "X= df[zmix_pca_dim_cols].values\n",
    "Y = df[\"souener\"].values\n",
    "from sklearn.utils import shuffle\n",
    "X_allSpecies_shuffled,X_shuffled, Y_shuffled = shuffle(X_allSpecies,X,Y, random_state=0)\n",
    "\n",
    "\n",
    "X_allSpecies_train = X_allSpecies_shuffled[::2]\n",
    "X_train = X_shuffled[::2]\n",
    "Y_train = Y_shuffled[::2]\n",
    "\n",
    "X_allSpecies_test = X_allSpecies_shuffled[1::2]\n",
    "X_test = X_shuffled[1::2]\n",
    "Y_test = Y_shuffled[1::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_scaledusingdm = dm.Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_unscaled = dm.Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-1d6a03519f31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# fit and transform in one step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mY_unscaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY_unscaled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_unscaled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mscaler_souener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_unscaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": [
    "scaler_souener = MinMaxScaler()\n",
    "# fit and transform in one step\n",
    "\n",
    "Y_unscaled = Y_unscaled.reshape(Y_unscaled.shape[0], 1)\n",
    "\n",
    "scaler_souener.fit_transform(Y_unscaled).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_scaledusingdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# create scaler\n",
    "scaler_species = MinMaxScaler()\n",
    "# fit and transform in one step\n",
    "normalized_species_train = scaler_species.fit_transform(X_allSpecies_train)\n",
    "normalized_species_test = scaler_species.fit_transform(X_allSpecies_test)\n",
    "\n",
    "# create scaler\n",
    "scaler_zmixpca = MinMaxScaler()\n",
    "# fit and transform in one step\n",
    "normalized_zmixpca_train = scaler_zmixpca.fit_transform(X_train)\n",
    "normalized_zmixpca_test = scaler_zmixpca.fit_transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "scaler_souener = MinMaxScaler()\n",
    "# fit and transform in one step\n",
    "\n",
    "Y_train = Y_train.reshape(Y_train.shape[0], 1)\n",
    "Y_test = Y_test.reshape(Y_test.shape[0], 1)\n",
    "\n",
    "normalized_souener_train = scaler_souener.fit_transform(Y_train)\n",
    "normalized_souener_test = scaler_souener.fit_transform(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_souener_train = normalized_souener_train.flatten()\n",
    "normalized_souener_test = normalized_souener_test.flatten()\n",
    "\n",
    "Y_train = Y_train.flatten()\n",
    "Y_test = Y_test.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_souener_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "species_inputs = keras.Input(shape=(53,), name=\"species_input\")\n",
    "\n",
    "linear_reduced_dims = layers.Dense(5, name=\"linear_layer\")(species_inputs)\n",
    "\n",
    "x = layers.Dense(32, activation=\"relu\")(linear_reduced_dims)\n",
    "x = layers.Dense(64, activation=\"relu\")(x)\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dense(256, activation=\"relu\")(x)\n",
    "x = layers.Dense(512, activation=\"relu\")(x)\n",
    "x = layers.Dense(256, activation=\"relu\")(x)\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dense(64, activation=\"relu\")(x)\n",
    "x = layers.Dense(32, activation=\"relu\")(x)\n",
    "#Predict the source energy\n",
    "souener_pred = layers.Dense(1, name=\"prediction\")(x)\n",
    "\n",
    "physics_pred = layers.Dense(5, name=\"physics\")(linear_reduced_dims)\n",
    "\n",
    "model = keras.Model(\n",
    "    inputs=[species_inputs],\n",
    "    outputs=[souener_pred, physics_pred],\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = keras.optimizers.Adam(learning_rate=0.01)\n",
    "model.compile(\n",
    "    optimizer=opt,\n",
    "    loss={\n",
    "        \"physics\": keras.losses.MeanAbsoluteError(),\n",
    "        \"prediction\": keras.losses.MeanAbsoluteError(),\n",
    "    },\n",
    "    loss_weights=[2.0, 0.2],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(dm.X_scaled_train, \n",
    "                    {\n",
    "                        \"physics\": dm.rom_scaled_train,\n",
    "                        \"prediction\":dm.Y_scaled_train\n",
    "                    },\n",
    "                    validation_split=0.2,\n",
    "                    verbose=0, \n",
    "                    epochs=100\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#batch_size=100,\n",
    "history = model.fit(normalized_species_train, \n",
    "                    {\n",
    "                        \"physics\": normalized_zmixpca_train,\n",
    "                        \"prediction\":normalized_souener_train#Y_train\n",
    "                    },\n",
    "                    validation_split=0.2,\n",
    "                    verbose=0, \n",
    "                    epochs=100\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_physics_and_regression(history):\n",
    "        \n",
    "    f = plt.figure(figsize=(10,3))\n",
    "    ax = f.add_subplot(121)\n",
    "    ax2 = f.add_subplot(122)\n",
    "    \n",
    "    ax.plot(history.history['prediction_loss'], label='loss')\n",
    "    ax.plot(history.history['val_prediction_loss'], label='val_loss')\n",
    "    ax.set_title('Souener Prediction Loss')\n",
    "    ax.set(xlabel='Epoch', ylabel='Souener Error')\n",
    "    ax.legend()\n",
    "\n",
    "    ax2.plot(history.history['physics_loss'], label='loss')\n",
    "    ax2.plot(history.history['val_physics_loss'], label='val_loss')\n",
    "    ax2.set_title('Physics Loss')\n",
    "    ax2.set(xlabel='Epoch', ylabel='Physics Error')\n",
    "    ax2.legend()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_physics_and_regression(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_physics_and_regression(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(dm.X_scaled_test)\n",
    "\n",
    "normalized_souener_pred = predictions[0]\n",
    "\n",
    "normalized_zmixpca_pred = predictions[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.createTrainTestData(\"ZmixPCA_randomequaltraintestsplit\",5, None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.X_scaled_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.rom_scaled_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(dm.Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = dm.outputScaler.inverse_transform(normalized_souener_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = Y_pred.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printError (err):\n",
    "    TotalAbsoluteError = err[0]\n",
    "\n",
    "    TotalSquaredError = err[1]\n",
    "\n",
    "    MeanAbsoluteError = err[2]\n",
    "\n",
    "    MeanSquaredError = err[3]\n",
    "\n",
    "    MeanPercentageError = err[4]\n",
    "\n",
    "    NumPoints = err[5]\n",
    "\n",
    "    print ('Total Absolute Error: ', TotalAbsoluteError)\n",
    "    print ('Mean Absolute Error: ', MeanAbsoluteError)\n",
    "    print ('Mean Percentage Error: ', MeanPercentageError)\n",
    "    print ('Total Squared Error: ', TotalSquaredError)\n",
    "    print ('Mean Squared Error: ', MeanSquaredError)\n",
    "    print ('Number of Points: ', NumPoints)\n",
    "\n",
    "def computeAndPrintError(Y_pred, Y_test):    \n",
    "    printError (computeError (Y_pred, Y_test))\n",
    "    return\n",
    "\n",
    "def computeError (Y_pred, Y_test):\n",
    "    evaluation_df_1 = pd.DataFrame()\n",
    "\n",
    "    evaluation_df_1['souener'] = Y_test.flatten()\n",
    "\n",
    "    evaluation_df_1['souener_pred'] = Y_pred.flatten()\n",
    "\n",
    "    evaluation_df_1['souener_pred_L1'] = evaluation_df_1['souener'] - evaluation_df_1['souener_pred'] \n",
    "\n",
    "    evaluation_df_1['souener_pred_L2'] = evaluation_df_1['souener_pred_L1'] * evaluation_df_1['souener_pred_L1']\n",
    "\n",
    "    evaluation_df_1['souener_pred_L1Percent'] = ((evaluation_df_1['souener'] - evaluation_df_1['souener_pred'])/evaluation_df_1['souener']) \n",
    "\n",
    "    TotalAbsoluteError = evaluation_df_1['souener_pred_L1'].abs().sum()\n",
    "\n",
    "    TotalSquaredError = evaluation_df_1['souener_pred_L2'].abs().sum()\n",
    "\n",
    "    MeanAbsoluteError = evaluation_df_1['souener_pred_L1'].abs().sum()/evaluation_df_1['souener_pred_L1'].abs().count()\n",
    "\n",
    "    MeanSquaredError = evaluation_df_1['souener_pred_L2'].abs().sum()/evaluation_df_1['souener_pred_L2'].abs().count()\n",
    "\n",
    "    NumPoints = evaluation_df_1['souener_pred_L1Percent'].abs().count()\n",
    "\n",
    "    MeanPercentageError = evaluation_df_1['souener_pred_L1Percent'].abs().sum()/NumPoints\n",
    "\n",
    "    return [TotalAbsoluteError,TotalSquaredError,MeanAbsoluteError,MeanSquaredError,MeanPercentageError,NumPoints]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err = computeAndPrintError(Y_pred, dm.Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.residplot(dm.Y_test, getResiduals(dm.Y_test,Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(normalized_species_test)\n",
    "\n",
    "normalized_souener_pred = predictions[0]\n",
    "\n",
    "normalized_zmixpca_pred = predictions[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = scaler_souener.inverse_transform(normalized_souener_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = Y_pred.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err = computeAndPrintError(Y_pred, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.residplot(Y_test.flatten(), getResiduals(Y_test,Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total Absolute Error:  6563454066133.884\n",
    "Mean Absolute Error:  798570880.4153649"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total Absolute Error:  4075696778120.0723\n",
    "Mean Absolute Error:  495887185.56029594"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"pcdnn_paper_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
