{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5daf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Wed Aug 4 17:50:06 2021\n",
    "\n",
    "@author: Amol & Dwyer\n",
    "\"\"\"\n",
    "\n",
    "# set TF GPU memory growth so that it doesn't hog everything at once\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from optuna_train import *\n",
    "debug_mode = True  # faster experiments for debugging\n",
    "\n",
    "def print_scientific_notation(number):\n",
    "    power = int(np.log(number)/np.log(10))\n",
    "    print(f\"Scientific Notation: {(loss/10**power)}*10^{power}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd16ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare the DataFrame that will be used downstream\n",
    "dp = DataPreparer()\n",
    "dp.createPCAs()\n",
    "dp.sparsePCAs()\n",
    "df = dp.getDataframe()\n",
    "\n",
    "# currently passing dp eventually we want to abstract all the constants into 1 class\n",
    "dm = DataManager(df, dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df226093",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" prepare PCDNNV2 for loading (from prior experiments) \"\"\"\n",
    "\n",
    "exprExec = PCDNNV2ExperimentExecutor()\n",
    "exprExec.debug_mode = debug_mode\n",
    "exprExec.setModelFactory(PCDNNV2ModelFactory())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d82d34",
   "metadata": {},
   "source": [
    "## Rapid Model Testing: \n",
    "### (requires setting up PCDNNV2 for loading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81165a2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fix seeds\n",
    "import random\n",
    "import numpy as np\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "dataType = 'randomequaltraintestsplit' #'frameworkincludedtrainexcludedtest'\n",
    "inputType = 'AllSpeciesAndZmix'\n",
    "dependants = 'NoDependants'\n",
    "dataSetMethod = f'{inputType}_{dataType}_{dependants}'\n",
    "opscaler=ipscaler=\"MinMaxScaler\"# 'PositiveLogNormal'\n",
    "#opscaler=None\n",
    "ZmixPresent = 'N'\n",
    "concatenateZmix = 'N'\n",
    "kernel_constraint = 'Y'\n",
    "kernel_regularizer = 'N'\n",
    "activity_regularizer = 'N'\n",
    "noOfCpv = 4\n",
    "noOfNeurons = 53\n",
    "\n",
    "exprExec.modelFactory.loss='mae'\n",
    "exprExec.modelFactory.activation_func='selu'\n",
    "exprExec.modelFactory.dropout_rate=0.0\n",
    "exprExec.debug_mode = False\n",
    "exprExec.use_dependants = True\n",
    "exprExec.use_dynamic_pred = True\n",
    "exprExec.epochs_override = 100\n",
    "exprExec.batch_size = 64\n",
    "exprExec.n_models_override = 1\n",
    "\n",
    "# initialize experiment executor...\n",
    "exprExec.dm = dm\n",
    "exprExec.df_experimentTracker = pd.DataFrame()\n",
    "exprExec.modelType = 'PCDNNV2'\n",
    "\n",
    "history = exprExec.executeSingleExperiment(noOfNeurons,dataSetMethod,dataType,inputType,ZmixPresent=ZmixPresent,\n",
    "                                           noOfCpv=noOfCpv,concatenateZmix=concatenateZmix,kernel_constraint=kernel_constraint,\n",
    "                                           kernel_regularizer=kernel_regularizer,activity_regularizer=activity_regularizer,\n",
    "                                           opscaler=opscaler, ipscaler=ipscaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a397193a",
   "metadata": {},
   "source": [
    "## Results Plotting & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125e242d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import model_analyzer.model_analysis as model_analysis\n",
    "import importlib; importlib.reload(model_analysis)\n",
    "\n",
    "bestModel, experimentSettings = exprExec.modelFactory.openBestModel()\n",
    "bestModel = exprExec.modelFactory.getEmbRegressor()\n",
    "#dm.createTrainTestData(experimentSettings['dataSetMethod'],experimentSettings['noOfCpv'], experimentSettings['ipscaler'], experimentSettings['opscaler'])\n",
    "dm = experimentSettings['data_manager']\n",
    "dp = dm.constants # god only knows why this is called \"constants\"\n",
    "history = experimentSettings['history']\n",
    "del experimentSettings['data_manager']\n",
    "del experimentSettings['history'] # remove clutter from print\n",
    "\n",
    "inspector = model_analysis.ModelInspector(exprExec.modelFactory, dm)\n",
    "print(f'\\nbestModel.input_shape: {bestModel.input_shape}')\n",
    "print(f'\\nexperimentSettings: {experimentSettings}')\n",
    "experimentSettings['history'] = history # put it pack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4202d56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(bestModel, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496ae24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(experimentSettings['history']['loss'][50:], color='blue')\n",
    "plt.plot(experimentSettings['history']['val_loss'][50:], color='red')\n",
    "plt.legend(labels=['loss', 'val_loss'])\n",
    "plt.title('Best model loss plot:')\n",
    "plt.show()\n",
    "print('Best Model dev R^2 (combined): ', experimentSettings['model_R2'])\n",
    "print('Best Model val static R^2:', experimentSettings['val_losses']['static_source_prediction_R2'])\n",
    "print('Best Model val dynamic R^2:', experimentSettings['val_losses']['dynamic_source_prediction_R2_split'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc47a66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_repeats = 5 if debug_mode else 20\n",
    "inspector.plot_permutation_feature_importance(n_repeats=n_repeats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7229e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "inspector.plot_partial_dependence()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da874389",
   "metadata": {},
   "source": [
    "#### Error Density by FlameId & XPos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2f6199",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getResiduals(Y,Y_pred):\n",
    "    return Y-Y_pred\n",
    "\n",
    "# TODO: Sanity check that this plots output ranges are correct? Seems like bug!\n",
    "bestModel = exprExec.modelFactory.getEmbRegressor()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# libraries and data\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "opscaler = dm.outputScaler\n",
    "df = dp.getDataframe()\n",
    "X, Y, Zmix, sources = dm.getAllData()\n",
    "Y_pred = bestModel.predict({\"species_input\":X, \"zmix\":Zmix})['static_source_prediction']\n",
    "\n",
    "# TODO: check should there be inverse transform here?\n",
    "souener_pred = opscaler.inverse_transform(Y_pred)[:, dm.souener_index]\n",
    "souener_actual = opscaler.inverse_transform(Y)[:, dm.souener_index]\n",
    "\n",
    "#residual/error\n",
    "residuals = getResiduals(souener_actual,souener_pred)\n",
    "Xpos = df['Xpos']\n",
    "\n",
    "plt_df = pd.DataFrame()\n",
    "plt_df[\"Xpos\"] = Xpos\n",
    "plt_df[\"Error\"] = residuals\n",
    "# plot\n",
    "plt.plot('Xpos', 'Error', data=plt_df, linestyle='', marker='o',markersize=1.5)\n",
    "plt.xlabel('Value of Xpos')\n",
    "plt.ylabel('Value of Error')\n",
    "plt.title('Residual, Scatterplot:', loc='left')\n",
    "\n",
    "# 2D density + marginal distribution:\n",
    "sns.jointplot(x=plt_df.Xpos, y=plt_df.Error, kind='kde')\n",
    "plt.title('Residual, Marginal Density plot:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8bab7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "flamekeys = df['flame_key']\n",
    "\n",
    "plt_df = pd.DataFrame()\n",
    "plt_df[\"Flamekeys\"] = flamekeys\n",
    "plt_df[\"Error\"] = residuals\n",
    "# plot\n",
    "plt.plot( 'Flamekeys', 'Error', data=plt_df, linestyle='', marker='o',markersize=1.5)\n",
    "plt.xlabel('Value of Flamekey')\n",
    "plt.ylabel('Value of Error')\n",
    "plt.title('Residual Scatterplot:', loc='left')\n",
    "\n",
    "# 2D density + marginal distribution:\n",
    "sns.jointplot(x=plt_df.Flamekeys, y=plt_df.Error, kind='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cf803a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(Y[:,0])\n",
    "plt.title('Souener (scaled) histogram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0234b497",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(Y_pred[:,0])\n",
    "plt.title('Souener_pred (scaled) histogram')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
